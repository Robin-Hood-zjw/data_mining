{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science & Linear Algebra in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will introduce you to some of the commonly used data science Python packages focusing on Pandas and Numpy. Also, as much of the foundation of data science is rooted in linear algebra, we will review matrix operations using Numpy. This is important as having a working understanding of matrices will be essential in this course as many of the algorithms we learn utilize and rely upon matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy\n",
    "import numpy as np\n",
    "from numpy.linalg import eig # https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html\n",
    "#%pip install pandas\n",
    "import pandas as pd\n",
    "#%pip install scikit-learn==0.23\n",
    "from sklearn.datasets import load_iris # used to obtain the pre-loaded data\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#%pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('classic')\n",
    "#%pip install scipy\n",
    "from scipy.linalg import svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "Pandas is a powerful data analysis and manipulation tool. When working with Pandas, we will typically be working with what is called a 'DataFrame,' which we can think of as a two-dimensional structure for storing tabular data. \n",
    "\n",
    "To showcase some of its functionality, we will load a dataset using another package, scikit-learn, which we will cover in detail in the next lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris() # load the data\n",
    "print(iris_data.keys()) # what does our data object contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use 'DESCR' to get more details about our data\n",
    "print(iris_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data into a pandas DataFrame using the 'data' and 'feature_names' keys\n",
    "iris_df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "iris_df.head() # preview the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many rows/columns does our data have? in other words, what is its shape?\n",
    "\n",
    "iris_df.shape # number of rows, number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will add the target variable, the species, to our DataFrame. To do so, we will make it into a **series** object, which is a one-dimensional array that can hold data of any type and is composed of indexes and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(iris_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add our target variable, the species to our DataFrame as a series object\n",
    "iris_df['species'] = pd.Series(iris_data.target)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we want our target variable to be the actual species name? We can use the replace method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing target variable\n",
    "iris_df['species'] = iris_df['species'].replace(to_replace= [0, 1, 2], value = iris_data.target_names)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be scenarios where we wish to filter to a subset of the data that meets certain conditions. This can be easily done using our pandas DataFrame. For example, what if we only want the rows where the species is 'setosa'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering data\n",
    "setosa_data= iris_df[(iris_df['species'] == 'setosa')] \n",
    "setosa_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify multiple conditions to filter on as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use & when we want both conditions to be met\n",
    "setosa_data_long_sepals = iris_df[(iris_df.species == 'setosa') & (iris_df['sepal length (cm)'] > 5.0)]\n",
    "setosa_data_long_sepals.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE: in the code above, replace '&' with '|' - can you tell the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also wish to sort our data by one of the columns. This can be done using the sort_values method on our dataframe to which we will specify the column on which we want to sort and whether we want to sort the values from low to high or high to low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting data, default is to sort values from low to high \n",
    "iris_df.sort_values('petal width (cm)').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE: add the argument 'ascending=False' to sort_values - what is the effect? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exploring our data, we usually want to get an understanding of the distribution of the values of our data. This can be done by applying the describe method to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 'describe' to get a better understanding of our features\n",
    "iris_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the nunique and unique methods to understand how many unique values exist in the column and what they are - this is especially useful for categorical data such as our 'species' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many different species? what are they?\n",
    "print(\"Number of unique species:\", iris_df.species.nunique())\n",
    "print(\"Species:\", iris_df.species.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the value_counts method to compute how many samples of each species we have in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of each species is in our data?\n",
    "iris_df.species.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to compute the mean/median/max/etc. of a column for each category/group? We can use the groupby method as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example; average petal length for each species\n",
    "iris_df.groupby('species')['petal length (cm)'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE: what is the maximum sepal width for each species?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful method is get_dummies which will create a dummy variable for each unique value/category of a column with a value of 0 or 1 to denote whether or not the instance belongs to that category. We can also use the drop_first argument if we want to create k-1 dummy variables where k is the number of unique categories - this is important when we need our columns to be linearly independent. We can apply this function to our species column as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = pd.get_dummies(iris_df, columns = [\"species\"]) \n",
    "dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE: add the argument 'drop_first = True' to the code above - does it behave as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may then choose to join these new columns to our original data. This can be done using the merge method. In the example below, we are joining the DataFrames on their indexes and are using an inner join meaning that we will only keep instances that match/exist in both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = iris_df.merge(dummy_data[['species_setosa', 'species_versicolor', 'species_virginica']], \n",
    "              how=\"inner\", left_index=True, right_index=True)\n",
    "\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always a good check to make sure we have the number of rows/columns we expected\n",
    "\n",
    "merged_data.shape # still have 150 rows, added 2/3 columns depending on which dummy variables were used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy\n",
    "\n",
    "Another popular Python library is Numpy. Numpy is great for working with arrays as it makes it easy and fast to apply functions to data that is formatted as an array. We can think of an array as a collection of elements that have associated indexes or positions, similar to a list. However, unlike in a list, array elements must all be of the same data type. We will use Numpy below to review matrices and matrix operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **matrix** is an array of numbers that are organized into a fixed number of rows and columns. We will start with the basics of matrices:\n",
    "- shape\n",
    "- transpose\n",
    "- mathematical operations (sum, mean, max, etc.)\n",
    "- matrix addition\n",
    "- element-wise multiplication\n",
    "- matrix multiplication\n",
    "- inverse\n",
    "- broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a matrix\n",
    "A = np.array([[1,2,3],[4,5,6]]) # use numpy to define our matrix\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Shape\n",
    "\n",
    "The shape of the matrix refers to how many rows and columns the matrix is composed of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of A:\", A.shape) # 2 rows, 3 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Transpose\n",
    "\n",
    "Transposing a matrix can be thought of as flipping the matrix about the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.T.shape) # see that we now have 3 rows, 2 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Mathematical Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying mathematical operations to our matrix, we can apply it to the entire matrix (the default) or specify the 'axis' - 0 will compute the operation for each column and 1 will compute the operation for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum\n",
    "print(A.sum()) # sums all entries of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "print(A.mean(axis=0)) # computes mean for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE: compute the maximum value for each row of matrix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Matrix Addition\n",
    "\n",
    "When adding matrices, it adds each element by its position. Therefore, the matrices must be the same shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([[1,3,5], [2,4,6]])\n",
    "print(A, \"\\n\")\n",
    "print(B)\n",
    "A + B "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Matix Element-Wise Multiplication\n",
    "\n",
    "Similar to adding matrices, we can multiple the elements of a matrix by multiplying each element that occupies the same position. Again, this means that the matrices must be of the same shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A*B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Matrix Multiplication\n",
    "\n",
    "You can use `@` to represent matrix multiplication. However, the size of the two arrays must be compatible to do this operation. For example, if A has shape (M, P) and matrix B has shape (P, N), they are considered compatible since the inner dimensions match (P=P). The result of `A @ B` will have shape (M, N) - the outer dimensions of the matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ B\n",
    "\n",
    "# think: why does this cause an error? consider their shapes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.shape, B.shape) # inner dimensions do not match, 3 != 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A.shape, B.T.shape) # inner dimensions now match, 3 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Inverse\n",
    "\n",
    "When working with numbers, an inverse is simply the reciprocal of the number. We can think of the inverse of a matrix in a similar fashion as it follows along a similar principal that $A × A^{-1} = A^{-1} × A = I$ where A is a matrix and I is the identity matrix. Therefore, we can think of the identity matrix as the matrix equivalent of \"1\" from when we multiple a number its by reciprocal. The resulting identity matrix will always be a square (i.e.., number of rows = number of columns) with values of 1 along the diagonal and 0 elsewhere. For more information on how the inverse is computed: https://www.mathsisfun.com/algebra/matrix-inverse.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of I, the identity matrix \n",
    "\n",
    "I = np.array([[1,0], [0,1]])\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = A @ B.T # matrix multiplication\n",
    "print(X, \"\\n\")\n",
    "X_inverse = np.linalg.inv(X)\n",
    "print(X_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks\n",
    "\n",
    "print(X @ X_inverse, \"\\n\")\n",
    "\n",
    "print(X_inverse @ X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 Broadcasting\n",
    "\n",
    "If arrays have different shapes but some dimensions are compatible, the operation will be broadcast to other dimensions. This allows us to still apply some arithmetic operations between arrays of different shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integers can be easily broadcast to the whole array\n",
    "print(A)\n",
    "print(A + 2) # adds two to each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE: multiple each element of matrix A by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcasting A to a matrix C of higher dimensions  \n",
    "C = np.array([[[1,1,1],[1,1,1]], [[2,2,2],[2,2,2]]])\n",
    "print(C.shape, A.shape) # second two dimensions are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A:\\n\", A, \"\\n\")\n",
    "print(\"C:\\n\", C, \"\\n\")\n",
    "print(\"A+C:\\n\", A + C) # adds A to both dimensions of C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Eigenvalues\n",
    "\n",
    "It is often useful to express things in simpler, smaller parts as properties of these simpler and smaller parts often speak volumes about the characteristics or behavior of the whole. They can also allow us to calculate things faster.\n",
    "\n",
    "This is where eigenvalues come in. **Eigenvalues** are values (positive and/or negative) that produce an original matrix when multiplied by a special vector, the **eigenvector**.\n",
    "\n",
    "Therefore, for a matrix A, eigenvector v and eigenvalues λ,\n",
    "    $Av = λv$, where:\n",
    "    - A is an n x n matrix, a “square” matrix **remember this requirement!**\n",
    "    - λ is a vector of length n \n",
    "    - V is also a vector of length n \n",
    "\n",
    "This allows us to represent big matrices with just two vectors! Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will fail, let's ask why?\n",
    "print(\"The shape of A is \", A.shape)\n",
    "try:\n",
    "    eig(A) # try to perform eigendecomposition\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"There was an error trying to do eigendecomposition on A!\")\n",
    "assert A.shape[0] == A.shape[1], \"This matrix is not square!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do matrices have to be square?\n",
    "$Av = \\lambda v$, the definition of the eigendecomposition problem only admits square shaped solutions.\n",
    "\n",
    "Imagine, we know A is of size $m \\times n$, then let's explore our definition:\n",
    "\n",
    "* [$m \\times n$] @ [$n \\times n$] = [$?$] * $n \\times n$\n",
    "\n",
    "* From the left side, we have matrix $A$ and eigenvectors $v$\n",
    "* Eigenvectors describe the column space of $A$, so we have $n$ of them\n",
    "* From the right side we have **scalar multiplication**, not a matrix product... \n",
    "* So what number must [$?$] be? From the basics we know we can only multiply $n$ scalars by $n$ rows, to produce a $n \\times n$ matrix\n",
    "* Ans: [?] must be of size $n$\n",
    "* And if this is true, then the left side, $m$ must actually be $m = n$ for it also to hold true\n",
    "\n",
    "caveat: Here I'm trying to intuitively convince you that the matrices have to be square, have not rigorously checked this argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_square_matrix = np.array([[5, 1], [3, 3]])\n",
    "the_eigenvalues, the_eigenvectors = eig(a_square_matrix)\n",
    "print(\"our square matrix: \\n\", a_square_matrix)\n",
    "print(\"our eigenvalues: \\n\", the_eigenvalues)\n",
    "print(\"our eigenvectors: \\n\", the_eigenvectors) # matrix of eigenvectors\n",
    "\n",
    "print(\"---- Let's check that one eigenvalue works as expected ----\")\n",
    "\n",
    "Av = a_square_matrix@the_eigenvectors # matrix multiplcation\n",
    "lambda_v = the_eigenvalues*the_eigenvectors # element-wise multiplication\n",
    "print('A@v\\n', Av)\n",
    "print(\"lambda * v\\n\", lambda_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Singular Value Decomposition (SVD)\n",
    "\n",
    "One of the major drawbacks of eigendecomposition is the requirement that the matrix must be square as we will frequently encounter situations where this is not the case. However, other matrix decomposition methods, such as **Singular Value Decomposition (SVD)** can be used in its place. SVD is similar to eigendecomposition in that an original matrix can be created from a function involving matrix products of eigenvectors and eigenvalues.  However, unlike eigendecomposition, it can work with non-square matrices and it is a partially constructed from matrix multiplication, left and right transpose and their eigenvectors. It is also different from eigendecomposition in that it is partially constructed from square roots of the eigenvalues, referred to as **singular values** rather than eigenvalues. We will refer to these singular values using 𝛔.\n",
    "\n",
    "So, for SVD we have 3 matrices of singular values: \n",
    "    - U - the left singular vectors\n",
    "    - V - the right singular vectors\n",
    "    - and the singular values common to both U and V, in a diagonal matrix 𝚺"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/\n",
    "\n",
    "U, s, VT = svd(a_square_matrix) # apply svd and get 3 resulting matrices\n",
    "\n",
    "# s contains singular values but we need to insert them into a proper matrix with the values along the diagonal\n",
    "Sigma = np.zeros((a_square_matrix.shape[0], a_square_matrix.shape[1])) # initalize matrix of all zeros\n",
    "Sigma[:A.shape[1], :A.shape[1]] = np.diag(s) # add values\n",
    "\n",
    "print(\"U\\n\", U)\n",
    "print(\"Sigma\\n\", Sigma)\n",
    "print(\"V transpose\\n\", VT)\n",
    "\n",
    "print(\"---- Let's see if A = U Sigma V' ----\\n\")\n",
    "\n",
    "print(\n",
    "    \"U Sigma V'\\n\",\n",
    "    U.dot(Sigma.dot(VT))\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Original Matrix\\n\",\n",
    "    a_square_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Principal Component Analysis (PCA)\n",
    "\n",
    "As review, variance is a measure of the spread or dispersion of data and is an important characteristic of data. With this in mind, PCA can be seen as a dimensionality reduction technique from N columns to k columns that preserves *most* of the variance among N columns. Comparing PCA and SVD, they are similar in that they both work on non-square and square matrices and make use of eigenvalues. However, PCA is different from SVD in that it does not work on the original matrix. Instead, it works on the covariance matrix.\n",
    "\n",
    "One important thing to keep in mind with PCA is that variables with more variability will get more \"power.\" Therefore, it is important that we scale our variables beforehand which can be done by simply setting whiten=True.\n",
    "\n",
    "Lastly, one thing you may be wondering is how to choose the optimal number of principal components, k. Some approaches include:\n",
    " - Chose manually: If you theoretically expect an underlying dimension of the dataset, like there were 3 factors that created a process, then this could be a choice\n",
    " - Scree plot: This can be somewhat subjective but it is simple, can pick a cumulative variance cutoff point\n",
    " - Cross-validate: If used in a predictive task, treat as a tuning/hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shamelessly taken from https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html\n",
    "# special note about this dataset: https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "\n",
    "iris = load_iris() # reload the data\n",
    "X = iris.data # features\n",
    "y = iris.target # target/outcome variable\n",
    "target_names = iris.target_names\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True) # apply PCA where k=2\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "# percentage of variance explained\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "# plot the results of our PCA\n",
    "plt.figure()\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "lw = 2\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of IRIS dataset - x axis is first component, y the second')\n",
    "plt.show() # see pretty good separation between the 3 species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results, we see that most of the variance is explained by the first principal component - this makes sense as our data is very limited in this example. However, with more complex datasets, having additional principal components will be important for capturing the additional variance in the data.\n",
    "\n",
    "In the next lab, we will build off of this introduction and will dive into applying machine learning techniques using scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
