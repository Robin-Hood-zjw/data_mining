{"cells":[{"cell_type":"markdown","metadata":{"id":"s9nNSgP7beDi"},"source":["# Lab 2: Introduction to Itemset Mining in Python"]},{"cell_type":"markdown","metadata":{"id":"wB7pNRRAbeDm"},"source":["In this lab, we will use a Groceries Dataset available on Kaggle. A full description of the dataset is available at https://www.kaggle.com/heeraldedhia/groceries-dataset. It is composed of 38,765 rows of items purchased at the grocery store and contains the following columns:\n","  * `Member_number` the id of the member who purchased the item\n","  * `Date` the date that the transaction occurred\n","  * `itemDescription` the item that was purchased\n","\n","\n","We will be performing Market Basket Analysis on the data by applying the **Apriori Algorithm**. When doing so, it is important to keep in mind that this is a very small, limited dataset so we must be careful with how we interpret the significance of any of our findings."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13158,"status":"ok","timestamp":1663283077909,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"},"user_tz":240},"id":"yGSscR0VbeDn","outputId":"3f370405-a290-46a2-9211-1326e763ac05"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting apyori\n","  Downloading apyori-1.1.2.tar.gz (8.6 kB)\n","Building wheels for collected packages: apyori\n","  Building wheel for apyori (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for apyori: filename=apyori-1.1.2-py3-none-any.whl size=5974 sha256=461cba6b752c1e3c839e49534bcdd30ffab0d0512344a293f24a11a5a2f08b09\n","  Stored in directory: /root/.cache/pip/wheels/cb/f6/e1/57973c631d27efd1a2f375bd6a83b2a616c4021f24aab84080\n","Successfully built apyori\n","Installing collected packages: apyori\n","Successfully installed apyori-1.1.2\n"]}],"source":["#%pip install pandas\n","import pandas as pd \n","#%pip install numpy\n","import numpy as np\n","#%pip install seaborn\n","import seaborn as sns\n","%pip install apyori\n","from apyori import apriori"]},{"cell_type":"markdown","metadata":{"id":"3B4JVfLpbeDo"},"source":["### 1. Data Exploration"]},{"cell_type":"markdown","metadata":{"id":"AVde5RuQbeDp"},"source":["Let's begin by loading the dataset and performing some preliminary data exploration."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":36},"id":"3Sx14P8SbeDp"},"outputs":[{"data":{"text/html":["\n","     \u003cinput type=\"file\" id=\"files-9eb884f7-c51c-4392-9c46-a0ca60316383\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" /\u003e\n","     \u003coutput id=\"result-9eb884f7-c51c-4392-9c46-a0ca60316383\"\u003e\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      \u003c/output\u003e\n","      \u003cscript\u003e// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) =\u003e {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable\u003c!Object\u003e} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) =\u003e {\n","    inputElement.addEventListener('change', (e) =\u003e {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) =\u003e {\n","    cancel.onclick = () =\u003e {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) =\u003e {\n","      const reader = new FileReader();\n","      reader.onload = (e) =\u003e {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position \u003c fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","\u003c/script\u003e "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-1-a8b38c6a9e41\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgroceries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Groceries_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m   \"\"\"\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 42\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 124\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    126\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"]}],"source":["# load the dataset\n","from google.colab import files\n","uploaded = files.upload()\n","\n","groceries = pd.read_csv('Groceries_dataset.csv')\n","print(\"Number of rows:\", groceries.shape[0])\n","print(\"Number of columns:\", groceries.shape[1])\n","groceries.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"qTCQf6u5beDp"},"source":["We see that we have 38,765 rows and three columns: the 'Member_number' (the ID of the customer), 'Date' (the date of the purchase) and 'itemDescription' (the product purchased). Therefore, each row represents an item that was purchased by a customer on a given date. If multiple items were purchased by the customer on that date, we will have multiple rows for that combination of 'Member_number' and 'Date'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfL1EXZPbeDq"},"outputs":[],"source":["# number of unique customers\n","print(\"Number of unique customers:\", groceries.Member_number.nunique())\n","# number of unique items\n","print(\"Number of unique items:\", groceries.itemDescription.nunique())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvczSvu9beDr"},"outputs":[],"source":["# number of unique dates\n","print(\"Number of unique dates:\", groceries.Date.nunique())\n","# range of dates\n","print(\"Date Range: {} - {}\".format(groceries.Date.min(), groceries.Date.max()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7vafZHfbeDr"},"outputs":[],"source":["# number of unique customer-date combination\n","print(\"Number of unique customer-date combinations:\", \n","      len(groceries.drop_duplicates(['Member_number', 'Date'])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htmA6MuubeDs"},"outputs":[],"source":["# average size of basket\n","avg_basket_size = np.round(groceries.groupby(['Member_number', 'Date']).size().mean(), 2)\n","min_basket_size = np.round(groceries.groupby(['Member_number', 'Date']).size().min(), 2)\n","max_basket_size = np.round(groceries.groupby(['Member_number', 'Date']).size().max(), 2)\n","print(\"Average Basket Size: {} items\".format(avg_basket_size))\n","print(\"Minimum Basket Size: {} items\".format(min_basket_size))\n","print(\"Maximum Basket Size: {} items\".format(max_basket_size))\n","\n","# plot distribution of basket sizes\n","sizes = groceries.groupby(['Member_number', 'Date']).size().reset_index(name='Basket Size')['Basket Size']\n","sns.distplot(sizes, kde=False).set_title('Distribution of Basket Sizes');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bufXNQU-beDs"},"outputs":[],"source":["# what are the 10 most frequent items in the dataset? how many times were they bought?\n","groceries.itemDescription.value_counts()[:10]"]},{"cell_type":"markdown","metadata":{"id":"03lT_NKAbeDt"},"source":["### 2. Association Rules"]},{"cell_type":"markdown","metadata":{"id":"e6WMU9hrbeDt"},"source":["Now that we have explored the nature of our data, we can start mining the data for insights. We will start with association rules. The end goal with association rules is to be able to predict other items that customers are likely to buy based on what they actually have bought/have in their basket. As we saw in lecture, an example of an association rule is that for people who bought {x, y, z}, they also tend to buy {v, w}. We will want to find all (interesting) rules X --\u003e Y with minimum support and confidence. As a review:\n","\n","  * `Support`: probability that a transaction contains X and Y\n","  * `Confidence`: conditional probability that a transaction having X also contains Y, P(Y|X)\n","  * `Interest`: difference between its confidence and the fraction of baskets that contain Y\n","  \n","Using a small subset of the data, we will first caclulate these concepts by hand. Then, we will apply the Apriori Algorithm to the entire dataset."]},{"cell_type":"markdown","metadata":{"id":"J2mccROLbeDu"},"source":["But first, we must do some data manipulation to get the data in the right format. We will need to group the data by 'Member_number' and 'Date' so that we have a set of the items for each transaction (i.e., all items purchased by the customer on that date)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dghLHf7-beDu"},"outputs":[],"source":["# get data in proper format\n","transactions = [set(items[1].itemDescription) for items in list(groceries.groupby(['Member_number','Date']))]\n","transactions[0] # example transaction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dncf5lCbeDu"},"outputs":[],"source":["# we will use a subset of 4 transactions for this example\n","\n","print(\"Transaction 1:\", transactions[0])\n","print(\"Transaction 2:\", transactions[6])\n","print(\"Transaction 3:\", transactions[14])\n","print(\"Transaction 4:\", transactions[58])\n","\n","transaction_subset = [transactions[0], transactions[6], transactions[14], transactions[24]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9pBbHkRYbeDv"},"outputs":[],"source":["# we can also represent this data as a dataframe/matrix where 1 = item present and 0 = item absent\n","\n","transactions_subset_df = pd.DataFrame({'semi-finished bread': [1,0,0,0],\n","                   'whole milk': [1,1,0,1],\n","                   'yogurt': [1,0,0,1],\n","                   'sausage': [1,1,1,1],\n","                   'rolls/buns': [0,1,1,0],\n","                   'bottled beer': [0,0,0,1] })\n","\n","transactions_subset_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0q_0hLkLbeDv"},"outputs":[],"source":["# calculate support for individual items: which are equal to or greater than minsup = 50%?\n","\n","item_counter = {}\n","for product in transactions_subset_df.columns:\n","    item_counter[product] = sum(transactions_subset_df[product]\u003e0)\n","    \n","item_counter"]},{"cell_type":"markdown","metadata":{"id":"SMIoTI4FbeDv"},"source":["Looking at the frequency of each individual item, we see that 'whole milk', and 'sausage' meet or surpass the 50% minsup threshold. Focusing in on these items, let's see if there are any pairs of items that surpass the 50% minsup threshold as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-egjHUvbeDw"},"outputs":[],"source":["# calculate support for item pairs: which are equal to or greater than minsup = 50%?\n","\n","# source: https://dzenanhamzic.com/2017/01/19/market-basket-analysis-mining-frequent-pairs-in-python/\n","\n","# take data matrix from dataframe\n","transaction_matrix = transactions_subset_df.to_numpy()\n","# get number of rows and columns\n","rows, columns = transaction_matrix.shape\n","# init new matrix\n","frequent_items_matrix = np.zeros((6,6))\n","# compare every product with every other\n","for this_column in range(0, columns-1):\n","    for next_column in range(this_column + 1, columns):\n","        # multiply product pair vectors\n","        product_vector = transaction_matrix[:,this_column] * transaction_matrix[:,next_column]\n","        # check the number of pair occurrences in baskets\n","        count_matches = sum((product_vector)\u003e0)\n","        # save values to new matrix\n","        frequent_items_matrix[this_column,next_column] = count_matches\n","\n","frequent_items_df = pd.DataFrame(frequent_items_matrix, columns = transactions_subset_df.columns.values, index = transactions_subset_df.columns.values)\n","frequent_items_df"]},{"cell_type":"markdown","metadata":{"id":"WEPRIdu5beDw"},"source":["Looking at the results, we see that the only itemset that surpasses our 50% minsup threshold is {whole milk, sausage}. Now let's calculate the confidence to see if it surpasses our 50% minconf threshold as well. \n","\n","Association rule 1: sausage -\u003e whole milk\n","\n","Confidence = Pr(whole milk | sausage) = 3/4 = 75%\n","\n","Interest = conf(sausage -\u003e whole milk) − Pr(whole milk) = 75% - 75% = 0% (definitely not interesting)\n","\n","\n","Association rule 2: whole milk -\u003e sausage\n","\n","Confidence = Pr(sausage | whole milk) = 3/3 = 100%\n","\n","Interest = conf(whole milk → sausage) − Pr(sausage) = 100% - 100% = 0% (also definitely not interesting)\n","\n","\n","This serves an an important lesson that not all high confidence association rules are interesting! Next, let's apply the apriori algorithm to our entire dataset to see if we can find more interesting insights."]},{"cell_type":"markdown","metadata":{"id":"mJSM5iqobeDw"},"source":["### 2.1 Association Rules using the Apriori Algorithm "]},{"cell_type":"markdown","metadata":{"id":"cWwsTiLfbeDx"},"source":["With these concepts in mind, we can specify the min_support and min_confidence when using the Apriori Algorithm. Also, we can specify the value for min_lift, a measure of interestingness. Keep in mind that we will want to try out different values of min_support, min_confidence and min_lift depending on the task and problem at hand - it will require a bit of trial and error."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BWpuHHI7beDx"},"outputs":[],"source":["association_results = list(apriori(transactions, min_support = 0.003, min_confidence = 0.05, min_lift=1.0))\n","association_results = filter(lambda x: len(x.items) \u003e 1, association_results) # filtering to rules with at least 2 items\n","\n","# source: https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/\n","for item in association_results:\n","    items = [x for x in item.items]\n","    print(\"Rule: \" + items[0] + \" -\u003e \" + items[1])\n","    print(\"Support: \" + str(item[1]))\n","    print(\"Confidence: \" + str(item[2][0][2]))\n","    print(\"Lift: \" + str(item[2][0][3]))\n","    print(\"=====================================\")"]},{"cell_type":"markdown","metadata":{"id":"uWSzi9XtbeDx"},"source":["Using the given criteria, we get a set of 8 association rules. We can see how some may be interesting. For example, we may predict that those who are buying bottled beer will also buy sausages since they could be planning a barbecue/party."]},{"cell_type":"markdown","metadata":{"id":"x7rNNo0HbeDx"},"source":["### 3. Frequent Itemsets"]},{"cell_type":"markdown","metadata":{"id":"8FWO9F1mbeDx"},"source":["Next, let's experiment with Finding Frequent Itemsets. We want to find items that are frequently purchased together in our groceries data. To do so, we must set a value for min_support, the minimum support needed for the itemset to be considered frequent. As a reminder, support refers to the fraction of baskets that contain the itemset. Therefore, an itemset will be considered frequent if its support value is greater than or equal to the min_support threshold we set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5-8sQ93pbeDx"},"outputs":[],"source":["itemsets = list(apriori(transactions, min_support=0.05)) # setting the minimum support value to 0.05\n","frequent_itemsets = []\n","\n","for item in itemsets:\n","    frequent_itemsets.append((item.items, item.support))\n","    \n","sorted_freq_itemsets = sorted(frequent_itemsets, key= lambda t:t[1], reverse=True)\n","\n","pd.DataFrame(sorted_freq_itemsets, columns=['Items', 'Support'])"]},{"cell_type":"markdown","metadata":{"id":"xFNH_bYDbeDy"},"source":["We see that all of our itemsets only include 1 item... this is not very interesting to us since we want to know what items customers tend to buy together. Let's up the value of min_support so that we get results with 2 or more items and then filter the itemsets to those with at least 2 items in them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TICIzi17beDy"},"outputs":[],"source":["itemsets = list(apriori(transactions, min_support=0.007))\n","itemsets = filter(lambda x: len(x.items) \u003e 1, itemsets)\n","\n","frequent_itemsets = []\n","for item in itemsets:\n","    frequent_itemsets.append((item.items, item.support))\n","    \n","sorted_freq_itemsets = sorted(frequent_itemsets, key= lambda t:t[1], reverse=True)\n","\n","pd.DataFrame(sorted_freq_itemsets, columns=['Items', 'Support'])"]},{"cell_type":"markdown","metadata":{"id":"IkXUx400beDy"},"source":["### 4. Evaluation of Frequent Itemsets"]},{"cell_type":"markdown","metadata":{"id":"Iqc819NvbeDy"},"source":["There are a variety of metrics available for evaluating frequent itemsets. Applying evaluation metrics is an important task since not all frequent itemsets will be meaningful. One example is Jaccard Similarity, which we will employ here. The entirety of the task is the following:\n","  * Split the dataset into seasons (Fall, Winter, Spring, Summer) and select 2 of your choice\n","  * Identify the 25 most frequent itemsets (with at least 2 items in each itemset) for each season\n","  * Compute the Jaccard Similarity between them"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWjmCpN7beDy"},"outputs":[],"source":["### YOUR CODE: divide the groceries dataframe into 4 dataframes, one for each season\n","### then, select 2 of them to use for the remainder of this analysis\n","\n","### you can use the following breakdown of months into seasons:\n","# Winter = December (12), January (01), February (02)\n","# Spring = March (03), April (04), May (05)\n","# Summer = June (06), July (07), August (08)\n","# Fall = September (09), October (10), November (11)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dx-xum9ibeDy"},"outputs":[],"source":["### YOUR CODE: transform the data into the proper format (hint: see code above)\n","def measure_Jaccord_distance(document1, document2):\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E0UfXwzwbeDz"},"outputs":[],"source":["### YOUR CODE: find the top 25 most frequent itemsets (with at least 2 items) for each season\n","### sort them by their value for 'support' to identify the top 25 itemsets\n","### HINT: adjust the value of min_support, as necessary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zs89xW42beDz"},"outputs":[],"source":["### YOUR CODE: compute the Jaccard Similarity between the two sets of the top 25 most frequent itemsets"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}