{"cells":[{"cell_type":"markdown","metadata":{"id":"revA2KVcaSkW"},"source":["# Lab 3: Introduction to Mining Matrix Data"]},{"cell_type":"markdown","metadata":{"id":"lN3RboC3aSkb"},"source":["In this lab, we will be focusing on **Dimensionality Reduction**. As discussed in lecture, Dimensionality Reduction can help us uncover insights by allowing us to \"zoom\" in on the important features in our data and discover hidden patterns. To show the power of this technique, we will be implementing a use case in which we will will apply **Latent Semantic Analysis (LSA)** to tweets about the H1N1 pandemic/vaccine. We can think of LSA as applying Singular Value Decomposition (SVD) to a Document x Term matrix. Then, we will shift our attention to recommender systems - we will apply SVD to a User x Movies dataset to find user-factors and movie-factors and will implement a simple recommender system using a clustering approach."]},{"cell_type":"code","source":["!pip install nltk\n","import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0CLJSedWwzkK","executionInfo":{"status":"ok","timestamp":1663910837673,"user_tz":240,"elapsed":3708,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"cf599262-6a79-4c0c-d306-cc3d7a4c0995"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"JAxTxHq4aSkc","executionInfo":{"status":"ok","timestamp":1663910833969,"user_tz":240,"elapsed":31168,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"d1067d6d-f592-4484-8c5e-9afedeec7769"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-01cc3af0-7a53-4f5f-aa2a-f9745bdbcaf7\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-01cc3af0-7a53-4f5f-aa2a-f9745bdbcaf7\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Lab3_Matrix-StarterCode.ipynb to Lab3_Matrix-StarterCode.ipynb\n","Saving links.csv to links.csv\n","Saving movies.csv to movies.csv\n","Saving ratings.csv to ratings.csv\n","Saving README.txt to README.txt\n","Saving tags.csv to tags.csv\n","Saving tweets_time1.json to tweets_time1.json\n","Saving tweets_time5.json to tweets_time5.json\n"]}],"source":["import json\n","import random\n","import re\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","from scipy import spatial\n","\n","from google.colab import files as colab_files\n","uploaded = colab_files.upload()"]},{"cell_type":"markdown","metadata":{"id":"iNSztoT-aSkd"},"source":["### 1.  LSA on H1N1 Twitter Data"]},{"cell_type":"markdown","metadata":{"id":"ZKGj63JRaSkd"},"source":["The first task is to leverage Twitter Data to gain insights into the evolution of vaccine acceptance during the H1N1 pandemic. As evident with our recent experiences with COVID-19, there is an increased sense of urgency to develop and distribute vaccines rapidly during pandemics. As such, H1N1 offers an interesting perspective as the H1N1 vaccine was developed in a very short period of time."]},{"cell_type":"markdown","metadata":{"id":"tdYfIlFDaSke"},"source":["Instead of looking at and comparing all of the H1N1 pandemic milestones (see below for a timeline of the milestones), we will focus in on just two of them:\n","\n","1) Milestone 1 - April 15: first human infection in California\n","\n","2) Milestone 5 - September 15: FDA announces approval of 4 H1N1 vaccines\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"-BlLFLTqaSke"},"source":["To complete this analysis, we must complete the following stelps:\n","\n","* **Step 1**: Use the Twitter API to retrieve ~500 tweets for each milestone\n","\n","    * Query definition: (H1N1 OR swine flu) AND (vaccine*), no retweets, only English tweets\n","    * **This step is already completed for you**  - you will just have to read in the tweet data from json files\n","    \n","\n","* **Step 2**: Explore and clean the tweets (e.g., remove links, stop words and numbers)\n","\n","\n","* **Step 3**: Apply TFIDFVectorizer to obtain the document-term matrix\n","\n","\n","* **Step 4**: Apply TruncatedSVD to get the topics for each milestone"]},{"cell_type":"markdown","metadata":{"id":"TkMQ5Q4maSkf"},"source":["#### 1.1 Load the Tweets Data"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6E-XD47ZaSkg","executionInfo":{"status":"ok","timestamp":1663910953137,"user_tz":240,"elapsed":81,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"f8b78019-e2d9-4edf-ef5d-c5206f544dce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Milestone 1 - Number of Tweets: 541\n","Milestone 5 - Number of Tweets: 500\n"]}],"source":["# we will load and analyze tweets from the two milestones separately\n","# tweets1 will refer to tweets from the 1st H1N1 milestone - 1st human infection in California\n","# tweets5 will refer to tweets from the 5th H1N1 milestone - FDA announces approval of 4 H1N1 vaccines\n","\n","with open('tweets_time1.json', 'r') as f:\n","    tweets1 = json.load(f)\n","    \n","print(\"Milestone 1 - Number of Tweets:\", len(tweets1))\n","    \n","with open('tweets_time5.json', 'r') as f:\n","    tweets5 = json.load(f)\n","    \n","print(\"Milestone 5 - Number of Tweets:\", len(tweets5))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FbmRASoraSkh","executionInfo":{"status":"ok","timestamp":1663910837674,"user_tz":240,"elapsed":6,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"72a7badb-ef29-43a4-f298-8fdce6dbbf33"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'timestamp': 'Thu Apr 23 22:12:33 +0000 2009',\n"," 'text': 'CDC: rare swine flu detected in 7 Americans. Still unknown if vaccine is available to protect against the strain. http://tinyurl.com/cksqp4'}"]},"metadata":{},"execution_count":5}],"source":["# example tweet object\n","# each tweet is stored with a unique id as the key and has the following attributes:\n","# (1) timestamp\n","# (2) text (this will be our focus)\n","\n","tweets1['1598228282']"]},{"cell_type":"markdown","metadata":{"id":"5vOut3_4aSkh"},"source":["#### 1.2 Explore and Clean Tweets"]},{"cell_type":"markdown","metadata":{"id":"_sI2CiTuaSki"},"source":["The first step will be to check for duplicate tweets in our data and remove them as necessary. This is important as we want to get a good distribution of the topics being discussed from the milestones and do not want them to be biased by duplicate tweets. "]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QORFKNYBaSki","executionInfo":{"status":"ok","timestamp":1663910837793,"user_tz":240,"elapsed":123,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"05106828-9f14-4fd1-859f-4793aeaf0c6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Before: 541 After: 527\n","Before: 500 After: 462\n"]}],"source":["# check for and remove any duplicates\n","\n","# source: https://www.w3schools.com/python/python_howto_remove_duplicates.asp\n","def remove_dups(x):\n","    return list(dict.fromkeys(x))\n","\n","tweets1_no_dups = []\n","tweets1_text = [tweets1[tweet][\"text\"] for tweet in tweets1] # extract the text from the tweet object\n","tweets1_no_dups = remove_dups(tweets1_text)\n","print(\"Before:\", len(tweets1), \"After:\", len(tweets1_no_dups))\n","\n","tweets5_no_dups = []\n","tweets5_text = [tweets5[tweet][\"text\"] for tweet in tweets5] # extract the text from the tweet object\n","tweets5_no_dups = remove_dups(tweets5_text)\n","print(\"Before:\", len(tweets5), \"After:\", len(tweets5_no_dups))"]},{"cell_type":"markdown","metadata":{"id":"tUfn6qEIaSki"},"source":["Note: we still may see substantial overlap in the content of some tweets; for example, tweets may refer to the same article by its title but we will no longer have tweets with the same *exact* content. Next, to famialize ourselves with the data, let's look at some example tweets from each milestone."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4RkIExRaSkj","executionInfo":{"status":"ok","timestamp":1663910837794,"user_tz":240,"elapsed":6,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"f7c1ffe1-d1ca-4329-a970-cd3b35383007"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample Tweets for Milestone 1\n","['Damn Swine Flu!!!Why did the CDC know about this years ago but this is the first we are told about it? Why is there no Vaccine??huh?why?why?', \"Vaccines 'will not stop swine flu': CURRENT flu vaccines will not stop a deadly virus spreading around the world.. http://tinyurl.com/dzywoj\", \"The swine Flu in Mexico was man made so that they could give all it's citizens a Vaccine shot it's coming to the USA as well! VACCINES!!!\", '@govchains \\nThose against TARP have been vaccinated against swine flu (pork barrel spending)', 'Swine flu vaccine poss ready by sep or oct via npr (via @edwardboches)', 'CDC press conference: very unlikely seasonal H1N1 vaccination effective against current outbreak. #swineflu', 'I heard Bruce Lee was born immune to Swine Flu...the only vaccine to cure it was in his blood.', 'Chuck Norris cured the Swine Flu by eating a cold pig whole as vaccination.', 'Researchers Working To Develop Swine Flu Vaccine: Last Friday, an e-mail containing the gene sequence from the S.. http://bit.ly/54Rv3', 'Vaccination for Swine Flu in 1976 linked to Guillain-Barré syndrome (GBS) http://tinyurl.com/dmavfg']\n","-------------------------------------------------------------------------------------------------------------\n","Sample Tweets for Milestone 5\n","['We asked you: Will you get a swine flu vaccine for you or your children? http://tinyurl.com/nzyrdt #swineflu #h1n1', 'CDC: 3.4 million inhalable H1N1 vaccine doses available soon - CNN.com http://bit.ly/ThBCr', 'Health Dept: H1N1 vaccine will be free for all http://bit.ly/4aBhD', \"#Singapore: 1m flu vaccines for S'pore: VACCINATIONS against the Influenza A (H1N1) virus could be av.. http://bit.ly/B91y9\", 'First Doses of Swine Flu Vaccine Coming Soon http://cli.gs/5uXUV', '2009 July David Icke on Swine flu +updates TIME2ACT against NWO Mafia Forced Vaccinations http://ff.im/-8m2Ry', 'FWR Update: Police and Military Train To Intern Swine Flu Vaccine Refusniks http://cli.gs/r64tJ', \"@Deltavogue Everytime we think we're slowing down w/H1N1 another epidemic breaks out. They do have a vaccine now for it\", 'Nine countries donate H1N1 pandemic vaccine - WHO http://bit.ly/HJ7MA', 'Studies: 1 dose of swine flu vaccine works (http://cli.gs/rBQzV) #LA #California']\n"]}],"source":["print(\"Sample Tweets for Milestone 1\")\n","print(random.sample(tweets1_no_dups, 10))\n","print(\"-------------------------------------------------------------------------------------------------------------\")\n","print(\"Sample Tweets for Milestone 5\")\n","print(random.sample(tweets5_no_dups, 10))"]},{"cell_type":"markdown","metadata":{"id":"6xg_kwjtaSkj"},"source":["Next, we will apply some pre-processing to the tweets. In any natural language processing task, this is an essential step but the pre-processing you apply will depend on the nature of the text. In this example, we will lowercase the text and remove links, stop words, numbers and the words that were included in the original query (the 'query words')."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"qDTxAKV1aSkj","executionInfo":{"status":"ok","timestamp":1663910837794,"user_tz":240,"elapsed":4,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# function to remove links\n","def strip_links(text):\n","    link_regex = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n","    links = re.findall(link_regex, text)\n","    for link in links:\n","        text = text.replace(link[0], ', ')\n","    return text\n","\n","# define stop words and query words list to be removed\n","stop_words = stopwords.words('english')\n","query_words = [\"H1N1\", \"swineflu\", \"swine\", \"flu\", \"vaccine\", \"vaccines\", \"vaccination\", \"vaccinations\", \n","                                                       \"vaccinate\", \"vaccinates\", \"vaccinated\"]"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Yb6fLaF4aSkk","executionInfo":{"status":"ok","timestamp":1663910837940,"user_tz":240,"elapsed":150,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["cleaned_tweets1 = []\n","\n","for tweet in tweets1_no_dups:\n","    tweet_no_link = strip_links(tweet) # strip off any links\n","    tweet_lower = tweet_no_link.lower() # lowercase all letters\n","    tweet_only_alpha = re.sub(\"[^a-zA-Z]\", \" \", tweet_lower) # remove all characters that are not alphabetical\n","    tweet_spaces_removed = re.sub(' +', ' ', tweet_only_alpha).strip() # remove extra spaces and strip any at beginning/end\n","    tweet_no_stop_words = [item for item in tweet_spaces_removed.split() if (item not in stop_words and item not in query_words)]\n","    cleaned_tweets1.append(\" \".join(tweet_no_stop_words))\n","\n","cleaned_tweets5 = []\n","\n","for tweet in tweets5_no_dups:\n","    tweet_no_link = strip_links(tweet) # strip off any links\n","    tweet_lower = tweet_no_link.lower() # lowercase all letters\n","    tweet_only_alpha = re.sub(\"[^a-zA-Z]\", \" \", tweet_lower) # remove all characters that are not alphabetical\n","    tweet_spaces_removed = re.sub(' +', ' ', tweet_only_alpha).strip() # remove extra spaces and strip any at beginning/end\n","    tweet_no_stop_words = [item for item in tweet_spaces_removed.split() if (item not in stop_words and item not in query_words)]\n","    cleaned_tweets5.append(\" \".join(tweet_no_stop_words))"]},{"cell_type":"markdown","metadata":{"id":"96VYP3RWaSkk"},"source":["#### 1.3 Extract the topics: Apply TFIDFVectorizer & TruncatedSVD"]},{"cell_type":"markdown","metadata":{"id":"s5FukJl1aSkk"},"source":["Now that we have cleaned the tweets, we can move on to extracting the topics! To do so, we must first apply TFIDFVectorizer to convert the tweets into a matrix of TF-IDF features. TF-IDF is a commonly used technique in natural language processing. It stands for term frequency–inverse document frequency. Basically, it is a technique used to compute the importance or relevance of each word by looking at how often a word appears in each text and how often it occurs across all texts in our corpus."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vlBg3B6NaSkl","executionInfo":{"status":"ok","timestamp":1663910838053,"user_tz":240,"elapsed":122,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"966d0cdb-a797-4c78-c7bd-1fb4886f5729"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of Milestone 1 Document-Term Matrix: (527, 1501)\n","Number of terms: 1501\n","Shape of Milestone 5 Document-Term Matrix: (462, 1086)\n","Number of terms: 1086\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}],"source":["# source: https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/\n","\n","# apply to milestone 1 \n","\n","vectorized_tweets1 = []\n","\n","vectorizer = TfidfVectorizer(stop_words='english', max_df = 0.5)  # ignore terms that have a document frequency higher than 0.5\n","vectorized_tweets1 = vectorizer.fit_transform(cleaned_tweets1) # apply to our cleaned tweets\n","print(\"Shape of Milestone 1 Document-Term Matrix:\", vectorized_tweets1.shape) # check shape of the document-term matrix\n","terms1 = vectorizer.get_feature_names()\n","print(\"Number of terms:\", len(terms1))\n","\n","# apply to milestone 5\n","\n","vectorized_tweets5 = []\n","\n","vectorizer = TfidfVectorizer(stop_words='english', max_df = 0.5)  # ignore terms that have a document frequency higher than 0.5\n","vectorized_tweets5 = vectorizer.fit_transform(cleaned_tweets5) # apply to our cleaned tweets\n","print(\"Shape of Milestone 5 Document-Term Matrix:\", vectorized_tweets5.shape) # check shape of the document-term matrix\n","terms5 = vectorizer.get_feature_names()\n","print(\"Number of terms:\", len(terms5))"]},{"cell_type":"markdown","metadata":{"id":"UkRbBPBcaSkl"},"source":["After applying TF-IDF to our tweets, we see that we get a document-term matrix for the tweets for each of our milestones. We see that the number of rows is the number of tweets and the number of columns is the number of terms. Next, we will apply TruncatedSVD to obtain our topics. In this example, we will set n_components to 5 since we want 5 topics for each milestone but this is a parameter that we will want to adjust given the task at hand."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"20ZelpxtaSkl","executionInfo":{"status":"ok","timestamp":1663910838171,"user_tz":240,"elapsed":120,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["terms_list1 = []\n","\n","svd_model = TruncatedSVD(n_components=5, random_state=671) # n_components = # of topics\n","svd_model.fit(vectorized_tweets1) # fit to our vectorized tweets\n","    \n","for i, comp in enumerate(svd_model.components_): # loop through our 5 topics\n","    terms_comp = zip(terms1, comp)\n","    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:20] # only take top 20 terms for each topic for simplicity\n","    terms_list1.append(sorted_terms)\n","\n","terms_list5 = []\n","\n","svd_model = TruncatedSVD(n_components=5, random_state=671) # n_components = # of topics\n","svd_model.fit(vectorized_tweets5) # fit to our vectorized tweets\n","    \n","for i, comp in enumerate(svd_model.components_): # loop through our 5 topics\n","    terms_comp = zip(terms5, comp) \n","    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:20] # only take top 20 terms for each topic for simplicity\n","    terms_list5.append(sorted_terms)"]},{"cell_type":"markdown","metadata":{"id":"7KAJjXgPaSkl"},"source":["Nice! Now let's take a look at our results!"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"mEcsp1CbaSkm","executionInfo":{"status":"ok","timestamp":1663910838172,"user_tz":240,"elapsed":8,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b4c276df-56c6-4d1b-c762-ca87c4ab1b41"},"outputs":[{"output_type":"stream","name":"stdout","text":["Milestone 1 Topics \n","\n","Topic 1: \n","['cdc', 'readies', 'case', 'pandemic', 'time', 'closely', 'novel', 'watching', 'seasonal', 'prepares', 'pat', 'sickening', 'source', 'feedzilla', 'new', 'help', 'prevents', 'memory', 'post', 'blog']\n","-------------------------------------------------------------------------------------------------\n","Topic 2: \n","['seasonal', 'help', 'health', 'officials', 'ap', 'say', 'news', 'pessimistic', 'protection', 'post', 'ingredient', 'national', 'protect', 'wants', 'pessimisti', 'seaso', 'aga', 'offer', 'protects', 'months']\n","-------------------------------------------------------------------------------------------------\n","Topic 3: \n","['months', 'ingredient', 'wants', 'news', 'mexico', 'ready', 'city', 'key', 'scientists', 'away', 'hope', 'health', 'launches', 'campaign', 'stop', 'com', 'livescience', 'roche', 'massive', 'gilead']\n","-------------------------------------------------------------------------------------------------\n","Topic 4: \n","['months', 'away', 'help', 'livescience', 'ryall', 'com', 'seasonal', 'develop', 'digitaljournal', 'distribute', 'possible', 'best', 'cnn', 'ready', 'major', 'obstacle', 'relian', 'states', 'united', 'production']\n","-------------------------------------------------------------------------------------------------\n","Topic 5: \n","['mexico', 'city', 'launches', 'campaign', 'massive', 'people', 'months', 'died', 'stop', 'current', 'protect', 'says', 'outbreak', 'huge', 'killed', 'baxter', 'xh', 'experts', 'propaganda', 'work']\n","-------------------------------------------------------------------------------------------------\n","\n"," Milestone 5 Topics \n","\n","Topic 1: \n","['share', 'countries', 'rich', 'fda', 'reuters', 'healthy', 'necessary', 'approves', 'new', 'globally', 'influenza', 'com', 'approved', 'examiner', 'says', 'agreed', 'washington', 'post', 'street', 'percent']\n","-------------------------------------------------------------------------------------------------\n","Topic 2: \n","['healthy', 'necessary', 'fda', 'approves', 'new', 'examiner', 'com', 'influenza', 'post', 'approved', 'according', 'street', 'journal', 'wall', 'virus', 'cbs', 'nod', 'news', 'applications', 'designed']\n","-------------------------------------------------------------------------------------------------\n","Topic 3: \n","['fda', 'approves', 'influenza', 'approved', 'new', 'street', 'journal', 'wall', 'virus', 'says', 'news', 'nod', 'cbs', 'applications', 'designed', 'approval', 'drug', 'administration', 'food', 'protect']\n","-------------------------------------------------------------------------------------------------\n","Topic 4: \n","['doses', 'soon', 'million', 'coming', 'cdc', 'available', 'october', 'inhalable', 'early', 'cnn', 'nasal', 'spray', 'st', 'set', 'health', 'flumis', 'com', 'nose', 'near', 'free']\n","-------------------------------------------------------------------------------------------------\n","Topic 5: \n","['women', 'pregnant', 'recommended', 'questioned', 'angeles', 'los', 'times', 'testing', 'vandy', 'approved', 'children', 'influenza', 'ozarks', 'lot', 'hearing', 'zionseekr', 'wsmv', 'emergence', 'evidence', 'shown']\n","-------------------------------------------------------------------------------------------------\n"]}],"source":["print(\"Milestone 1 Topics \\n\")\n","sub_count = 1\n","for topic in terms_list1:\n","    print(\"Topic \"+str(sub_count)+\": \")\n","    print([t[0] for t in topic])\n","    sub_count += 1\n","    print(\"-------------------------------------------------------------------------------------------------\")\n","\n","print(\"\\n Milestone 5 Topics \\n\")\n","sub_count = 1\n","for topic in terms_list5:\n","    print(\"Topic \"+str(sub_count)+\": \")\n","    print([t[0] for t in topic])\n","    sub_count += 1\n","    print(\"-------------------------------------------------------------------------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"TzBhnT8OaSkm"},"source":["Looking at the topics, we can start to get an idea of how we could assign 'topic names' to each of these. For example, for topic 1 from milestone 1, we may label it 'National Preparation for Emerging Vaccine' given the words in the topic. "]},{"cell_type":"code","execution_count":13,"metadata":{"id":"XhPAGMfhaSkm","executionInfo":{"status":"ok","timestamp":1663910838268,"user_tz":240,"elapsed":99,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d2cf1bed-1b6a-4d5d-b244-0f5b5331a40d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["TruncatedSVD(n_components=3, random_state=671)"]},"metadata":{},"execution_count":13}],"source":["### YOUR CODE: apply TruncatedSVD to the tweets with your choice of n_components (not 5!)\n","### how do the results differ? what do you think the 'best' choice of n_components is for this data?\n","svd_model = TruncatedSVD(n_components=3, random_state=671)\n","svd_model.fit(vectorized_tweets5)"]},{"cell_type":"markdown","metadata":{"id":"tj3dxbXwaSkm"},"source":["From here, there are a variety of next steps and analyses that we could perform. For example, we may want to look at the similarity of topics between the different milestones or we may want to perform additional analyses such as sentiment anaylsis to combine with these findings. But in this lab, we will stop here and will move on to our next example: appying SVD to a Users-Movies dataset."]},{"cell_type":"markdown","metadata":{"id":"5BHC7YuJaSkm"},"source":["### 2. Users-Movie Dataset"]},{"cell_type":"markdown","metadata":{"id":"d2_wQnjuaSkm"},"source":["First, we must download the data. Navigate to https://grouplens.org/datasets/movielens/ and download the 'ml-latest-small.zip' file under MovieLens Latest Datasets. This dataset contains 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users. We will focus on:\n","   * 'movies.csv' - contains the 'movieId', 'title' and 'genres'\n","   * 'ratings.csv' - contains the 'userId', 'movieId', 'rating', 'timestamp'\n","   \n","Dataset citation: F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. https://doi.org/10.1145/2827872 "]},{"cell_type":"code","execution_count":14,"metadata":{"id":"XT1deazkaSkn","executionInfo":{"status":"error","timestamp":1663910838599,"user_tz":240,"elapsed":334,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"colab":{"base_uri":"https://localhost:8080/","height":364},"outputId":"dba43784-a348-4c0c-9f79-5576b25efded"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-13cd08920c06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# movies data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmovies_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ml-latest-small/movies.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmovies_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ml-latest-small/movies.csv'"]}],"source":["# movies data\n","movies_data = pd.read_csv('ml-latest-small/movies.csv')\n","print(movies_data.shape)\n","movies_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xewDGifRaSkn","executionInfo":{"status":"aborted","timestamp":1663910838600,"user_tz":240,"elapsed":6,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# users data\n","users_data = pd.read_csv('ml-latest-small/ratings.csv')\n","print(users_data.shape)\n","users_data.head()"]},{"cell_type":"markdown","metadata":{"id":"nAbljkizaSkn"},"source":["#### 2.1 Explore the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWSD_UBsaSkn","executionInfo":{"status":"aborted","timestamp":1663910838601,"user_tz":240,"elapsed":7,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# YOUR CODE: explore the data! some ideas for what you may want to consider include:\n","# (1) how many users are there?\n","# (2) how many movies are there with at least 1 rating? \n","# (3) on average, how many movies has each user rated?\n","# (4) what does the distribution of ratings look like?"]},{"cell_type":"markdown","metadata":{"id":"ZBgp-iS1aSkn"},"source":["#### 2.2 Apply SVD to Obtain Factors"]},{"cell_type":"markdown","metadata":{"id":"KIqWDQvJaSko"},"source":["Before we can apply SVD to our data, we must transform it into an appropiate format. To do so, we will use the pivot function where the index='userId' so we will have one row for each user, columns='movieId' so we will have one column for each movie and values='ratings' to fill each value in the table. We will use 0 to indicate movies for which the user has not provided a rating. As the average user has rated 165.3 movies, we will expect the majority of the values to be equal to 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IXoXgRPvaSko","executionInfo":{"status":"aborted","timestamp":1663910838601,"user_tz":240,"elapsed":6,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["pivoted_users_df = users_data.pivot(index = 'userId', columns ='movieId', values = 'rating').fillna(0)\n","print(\"Shape:\", pivoted_users_df.shape) #check\n","pivoted_users_df.head()"]},{"cell_type":"markdown","metadata":{"id":"8K5T8OZYaSko"},"source":["As we have many movies that have only been rated by a handful of users, we are going to focus on the 500 most-rated movies. Therefore, we must find the names of the 500 most rated movies and filter to the subset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqLJ3AOeaSko","executionInfo":{"status":"aborted","timestamp":1663910838602,"user_tz":240,"elapsed":31338,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# find the names of the 500 most rated movies\n","\n","movieIds = pivoted_users_df.columns\n","movieNames = [movies_data[movies_data.movieId == id_]['title'].iloc[0] for id_ in movieIds] # extract movie titles\n","pivoted_users_df.columns = movieNames\n","counts = pivoted_users_df.apply(np.count_nonzero, axis=0) # count the number of ratings for each movie\n","number_ratings_df = pd.DataFrame(counts).reset_index()\n","number_ratings_df.columns = ['movieName', 'numberRatings']\n","# get the names of the 500 movies with the most ratings in the df\n","movieNames_500 = number_ratings_df.sort_values('numberRatings', ascending=False)[0:500].movieName\n","len(movieNames_500) # check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0hrlfghgaSko","executionInfo":{"status":"aborted","timestamp":1663910838603,"user_tz":240,"elapsed":31338,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# get the dataframe for these 500 movies only\n","movies500_ratings = pivoted_users_df[list(movieNames_500)]\n","movies500_ratings\n","# we have 501 rows - looks like we may have a duplicate movie? something to keep in mind..."]},{"cell_type":"markdown","metadata":{"id":"U7v9fvpwaSkp"},"source":["Now that the data is in the proper format, we can apply SVD. We will get three items returned:\n","\n","1) U:  user-to-concept similarity matrix\n","\n","2) Σ (sigma): the diagonal elements representing the ‘strength’ of each concept\n","\n","3) V: movie-to-concept similarity matrix\n","\n","In this example, we will extract the 50 latent factors with the greatest 'strength' values. \n","\n","**Thought exercise**: what do you expect to be the shapes of U, Σ and V?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QI2FV7bkaSkp","executionInfo":{"status":"aborted","timestamp":1663910838712,"user_tz":240,"elapsed":5,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["svd_model = TruncatedSVD(n_components=50, random_state=671)\n","users_df_svd = svd_model.fit_transform(movies500_ratings)\n","\n","U = users_df_svd / svd_model.singular_values_ \n","sigma = svd_model.singular_values_\n","V = svd_model.components_\n","\n","# checks \n","print(\"Shape of U:\", U.shape)\n","print(\"Length of Sigma:\", len(sigma))\n","print(\"Shape of V:\", V.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"FoKYXUKeaSkp","executionInfo":{"status":"aborted","timestamp":1663910838712,"user_tz":240,"elapsed":5,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["sigma # in decreasing order"]},{"cell_type":"markdown","metadata":{"id":"K_4re6D7aSkp"},"source":["Now that we have decomposed our matrix, one question we may want to answer is: what movies are clustering together? To begin answering this, let's look at some of the latent factors for the movies and see if we can get an idea of what the factors may represent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBQlOWM3aSkp","executionInfo":{"status":"aborted","timestamp":1663910838713,"user_tz":240,"elapsed":5,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["movie_concept1 = V[0] # the concept with the greatest strength, just the average (not very helpful)\n","movieNames = movies500_ratings.columns # get a list of the movie names\n","indices = sorted(range(len(movie_concept1)), key=lambda i: movie_concept1[i], reverse=True)[0:20] # get the indices of the movies with the top 20 values\n","names = [movieNames[ind] for ind in indices] # get the corresponding movies\n","movies_data[movies_data.title.isin(names)] \n","# we see lots of classics/popular movies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqy60kzBaSkq","executionInfo":{"status":"aborted","timestamp":1663910838713,"user_tz":240,"elapsed":5,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["movie_concept1 = V[1] # the concept with the 2nd greatest strength\n","movieNames = movies500_ratings.columns # get a list of the movie names\n","indices = sorted(range(len(movie_concept1)), key=lambda i: movie_concept1[i], reverse=True)[0:20] # get the indices of the movies with the top 20 values\n","names = [movieNames[ind] for ind in indices] # get the corresponding movie index\n","movies_data[movies_data.title.isin(names)] \n","# we see lots of action/adventure movies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAytNT9HaSkq","executionInfo":{"status":"aborted","timestamp":1663910838713,"user_tz":240,"elapsed":5,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["### YOUR CODE: pick some other movie concepts and try to determine what they could be representing"]},{"cell_type":"markdown","metadata":{"id":"HP0HpwbQaSkq"},"source":["As you can see, it is not always exactly clear what the latent factors represent but we can make some hypotheses about why certain movies are being clustered together. Next, we will see how we can use the matrix V to calculate the similarity between movies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"roZBfAEYaSkq","executionInfo":{"status":"aborted","timestamp":1663910838714,"user_tz":240,"elapsed":31438,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# transform V into a dataframe with the movie titles as the columns\n","movie_factors_df = pd.DataFrame(V, columns = movies500_ratings.columns)\n","movie_factors_df.shape #check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3XLU6aoaSkq","executionInfo":{"status":"aborted","timestamp":1663910838714,"user_tz":240,"elapsed":31435,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# calculate the similarity between movies using cosine similarity/distance\n","movie1 = movie_factors_df['Beauty and the Beast (1991)']\n","movie2 = movie_factors_df['Aladdin (1992)']\n","movie3 = movie_factors_df['Godfather, The (1972)']\n","result = 1 - spatial.distance.cosine(movie1, movie2)\n","print(\"Beauty and the Beast vs. Aladdin:\", result)\n","result = 1 - spatial.distance.cosine(movie1, movie3)\n","print(\"Beauty and the Beast vs. The Godfather:\", result)\n","result = 1 - spatial.distance.cosine(movie2, movie3)\n","print(\"Aladdin Beat vs. The Godfather:\", result)"]},{"cell_type":"markdown","metadata":{"id":"YkiCTPoFaSkq"},"source":["As an example, we see that Beauty and the Beast and Aladdin are much more similar to one another than they are to a serious film like the Godfather."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uldKNeQgaSkq","executionInfo":{"status":"aborted","timestamp":1663910838715,"user_tz":240,"elapsed":31434,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["### YOUR CODE: select a few movies of your choice and calculate the similarity between them"]},{"cell_type":"markdown","metadata":{"id":"M393sNBvaSkr"},"source":["#### 2.3 Simple Recommendation System\n","\n","Lastly, we will see how we can build a simple movie recommendation system using K-means clustering. We will apply this to the user factors matrix we got by applying SVD above. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGD0NDaTaSkr","executionInfo":{"status":"aborted","timestamp":1663910838715,"user_tz":240,"elapsed":31432,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# cluster based on user factors - goal is to find similar users\n","\n","# apply clustering algorithm\n","from sklearn.cluster import KMeans\n","clusters = KMeans(n_clusters=5, random_state=6).fit_predict(U) # arbitrary choice of k\n","clustered_users_df = pd.DataFrame(U, index = pivoted_users_df.index)\n","clustered_users_df['cluster'] = clusters # create new column with cluster membership\n","print(clustered_users_df.shape) # check\n","clustered_users_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OO6tGs6naSkr","executionInfo":{"status":"aborted","timestamp":1663910838716,"user_tz":240,"elapsed":31432,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# distribution of users among clusters\n","clustered_users_df.cluster.value_counts()\n","# see that the majority of users are in cluster 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNz8d-O9aSkr","executionInfo":{"status":"aborted","timestamp":1663910838716,"user_tz":240,"elapsed":31430,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# who are the similar users?\n","\n","# select a user\n","user = int(input(\"Enter a user id: \")) \n","\n","cluster_no = int(clustered_users_df[clustered_users_df.index == user].cluster) # get the user's cluster\n","print(\"User\", user, \"belongs to cluster\", cluster_no)\n","similar_users = list(clustered_users_df[clustered_users_df.cluster == cluster_no].index) # find other users in the cluster\n","print(\"Number of similar users:\", len(similar_users))\n","print(\"Similar users:\", similar_users)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l1XwiPxVaSkr","executionInfo":{"status":"aborted","timestamp":1663910838716,"user_tz":240,"elapsed":31428,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# how has the cluster rated the movies in our dataset?\n","\n","cluster_df = movies500_ratings.loc[similar_users] # filter to users in cluster\n","cluster_df[cluster_df == 0] = np.nan # set 0 values to NaN (to not bias calculation of means)\n","avg_ratings = cluster_df.mean() # get the mean rating for each movie for the cluster\n","avg_ratings.sample(5) # preview a sample of the mean cluster ratings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dg5lLAzmaSkr","executionInfo":{"status":"aborted","timestamp":1663910838717,"user_tz":240,"elapsed":31427,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# what are top 10 rated movies for the cluster?\n","\n","avg_ratings_df = pd.DataFrame(avg_ratings).reset_index()\n","avg_ratings_df.columns = ['movieName', 'avgClusterRating']\n","avg_ratings_df.sort_values('avgClusterRating', ascending=False).head(10)"]},{"cell_type":"markdown","metadata":{"id":"nTP5sejtaSks"},"source":["The above list tells us the 10 movies with the highest average ratings across the cluster's users. However, when we recommend movies to our user, we want to make sure they haven't seen it before, so we must filter those out from our recommendations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0uq_w7laSks","executionInfo":{"status":"aborted","timestamp":1663910838717,"user_tz":240,"elapsed":31425,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["### YOUR CODE: what are top 10 rated movies for the cluster (that our user has not rated previously)?"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"video-info":{"admin":["localhost:8888"],"hub-user":"localhost:8888","path":"Labs>>Lab 3 - Matrix Mining/Lab3_Matrix-StarterCode.ipynb","tpc":"localhost:8888@Labs>>Lab 3 - Matrix Mining/Lab3_Matrix-StarterCode.ipynb"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}