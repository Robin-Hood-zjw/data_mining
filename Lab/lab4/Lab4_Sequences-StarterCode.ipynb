{"cells":[{"cell_type":"markdown","metadata":{"id":"lljQHnGVgw6P"},"source":["# Lab 4: Introduction to Mining Sequence Data"]},{"cell_type":"markdown","metadata":{"id":"xCNI8AZ9gw6T"},"source":["In this lab, we will first focus on Sequence Comparison/Alignment. We will compute the Levenshtein Edit Distance for country names and implement a simple version of BLAST to compare genomes. Then, we will turn to Sequence Prediction/Labeling and will walk through a Named Entity Extraction task."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18807,"status":"ok","timestamp":1664477329953,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"},"user_tz":240},"id":"JfMr2d7Tgw6U","outputId":"f3b0c02b-08e7-41f2-92e3-25167a7a06dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: country_list in /usr/local/lib/python3.7/dist-packages (1.0.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: Bio in /usr/local/lib/python3.7/dist-packages (1.4.0)\n","Requirement already satisfied: biopython>=1.79 in /usr/local/lib/python3.7/dist-packages (from Bio) (1.79)\n","Requirement already satisfied: mygene in /usr/local/lib/python3.7/dist-packages (from Bio) (3.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from Bio) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from Bio) (4.64.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython>=1.79->Bio) (1.21.6)\n","Requirement already satisfied: biothings-client>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from mygene->Bio) (0.2.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pomegranate in /usr/local/lib/python3.7/dist-packages (0.14.8)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pomegranate) (1.21.6)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pomegranate) (1.7.3)\n","Requirement already satisfied: joblib>=0.9.0b4 in /usr/local/lib/python3.7/dist-packages (from pomegranate) (1.1.0)\n","Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from pomegranate) (2.6.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from pomegranate) (6.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.7/dist-packages (0.3.6)\n","Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (0.9.8)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (0.8.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (1.15.0)\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (4.64.1)\n"]}],"source":["%pip install numpy --upgrade\n","import numpy as np\n","import pandas as pd\n","\n","%pip install country_list\n","from country_list import countries_for_language\n","\n","%pip install Bio\n","from Bio.Blast import NCBIWWW, NCBIXML # calling online version\n","\n","#%pip install sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.dummy import DummyClassifier\n","from sklearn.metrics import classification_report\n","\n","%pip install pomegranate\n","# may need to run 'pip install pomegranate --no-cache-dir --no-binary :all:' depending on setup\n","from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n","# if when running import, give an error saying something about numpy version:\n","#%pip install numpy --upgrade\n","# and then restart kernel\n","\n","%pip install sklearn_crfsuite\n","import sklearn_crfsuite\n","from sklearn_crfsuite import scorers\n","from sklearn_crfsuite import metrics\n","\n","from collections import Counter, defaultdict"]},{"cell_type":"markdown","metadata":{"id":"4CU9gdADgw6V"},"source":["## 1. Sequence Comparison/Alignment"]},{"cell_type":"markdown","metadata":{"id":"t3_nqerPgw6W"},"source":["### 1.1 Levenshtein Edit Distance\n","\n","Levenshtein Edit Distance helps us assess the similarity of sequences. It is calculated in terms of the minimum amount of operations (insertion, deletions and substitutions) that must be made to make two sequences identical. The smaller the distance, the greater the similarity between the sequences. To show how it is computed, we can generate a matrix and find the \"trace\" or the path for the minimum edit distance."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"5NM0jeXrgw6W","executionInfo":{"status":"ok","timestamp":1664477329953,"user_tz":240,"elapsed":11,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# source: https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/\n","\n","def levenshtein(seq1, seq2):\n","    size_x = len(seq1) + 1\n","    size_y = len(seq2) + 1\n","    matrix = np.zeros((size_x, size_y), dtype=\"object\") # changed dtype\n","    for x in range(size_x):\n","        matrix [x, 0] = x\n","    for y in range(size_y):\n","        matrix [0, y] = y\n","\n","    for x in range(1, size_x):\n","        for y in range(1, size_y):\n","            if seq1[x-1] == seq2[y-1]:\n","                matrix [x,y] = min(\n","                    matrix[x-1, y] + 1,\n","                    matrix[x-1, y-1],\n","                    matrix[x, y-1] + 1\n","                )\n","            else:\n","                matrix [x,y] = min(\n","                    matrix[x-1,y] + 1,\n","                    matrix[x-1,y-1] + 1,\n","                    matrix[x,y-1] + 1\n","                )\n","    \n","    # added code for readability\n","    matrix[0,0] = \"\"\n","    for x in range(1, size_x):\n","        matrix [x, 0] = seq1[x-1]\n","    for y in range(1, size_y):\n","        matrix [0, y] = seq2[y-1]\n","        \n","    print(matrix)\n","    \n","    return matrix[size_x - 1, size_y - 1]"]},{"cell_type":"markdown","metadata":{"id":"3Q88GIjygw6X"},"source":["First, to test this function, let's try out the example from class to compute the distance between \"William Cohen\" and \"William Cohon\"."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"40-C9_ozgw6X","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664477329954,"user_tz":240,"elapsed":11,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"ae484abb-6fe8-41ca-ec46-f05f3358ed65"},"outputs":[{"output_type":"stream","name":"stdout","text":["[['' 'W' 'i' 'l' 'l' 'l' 'i' 'a' 'm' ' ' 'C' 'o' 'h' 'o' 'n']\n"," ['W' 0 1 2 3 4 5 6 7 8 9 10 11 12 13]\n"," ['i' 1 0 1 2 3 4 5 6 7 8 9 10 11 12]\n"," ['l' 2 1 0 1 2 3 4 5 6 7 8 9 10 11]\n"," ['l' 3 2 1 0 1 2 3 4 5 6 7 8 9 10]\n"," ['i' 4 3 2 1 1 1 2 3 4 5 6 7 8 9]\n"," ['a' 5 4 3 2 2 2 1 2 3 4 5 6 7 8]\n"," ['m' 6 5 4 3 3 3 2 1 2 3 4 5 6 7]\n"," [' ' 7 6 5 4 4 4 3 2 1 2 3 4 5 6]\n"," ['C' 8 7 6 5 5 5 4 3 2 1 2 3 4 5]\n"," ['o' 9 8 7 6 6 6 5 4 3 2 1 2 3 4]\n"," ['h' 10 9 8 7 7 7 6 5 4 3 2 1 2 3]\n"," ['e' 11 10 9 8 8 8 7 6 5 4 3 2 2 3]\n"," ['n' 12 11 10 9 9 9 8 7 6 5 4 3 3 2]]\n","Edit Distance = 2\n"]}],"source":["print(\"Edit Distance =\", levenshtein(\"William Cohen\", \"Willliam Cohon\"))"]},{"cell_type":"markdown","metadata":{"id":"pCSaGEYjgw6Y"},"source":["We see that it matches the result from class. Next, let's try an example with countries. First, let's comment out the print command from the function above to reduce the size of our output."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"a8QoqSR7gw6Y","executionInfo":{"status":"ok","timestamp":1664477329954,"user_tz":240,"elapsed":8,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# source: https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/\n","def levenshtein(seq1, seq2):\n","    size_x = len(seq1) + 1\n","    size_y = len(seq2) + 1\n","    matrix = np.zeros((size_x, size_y), dtype=\"object\") # changed dtype\n","    for x in range(size_x):\n","        matrix [x, 0] = x\n","    for y in range(size_y):\n","        matrix [0, y] = y\n","\n","    for x in range(1, size_x):\n","        for y in range(1, size_y):\n","            if seq1[x-1] == seq2[y-1]:\n","                matrix [x,y] = min(\n","                    matrix[x-1, y] + 1,\n","                    matrix[x-1, y-1],\n","                    matrix[x, y-1] + 1\n","                )\n","            else:\n","                matrix [x,y] = min(\n","                    matrix[x-1,y] + 1,\n","                    matrix[x-1,y-1] + 1,\n","                    matrix[x,y-1] + 1\n","                )\n","    \n","    # added code for readability\n","    matrix[0,0] = \"\"\n","    for x in range(1, size_x):\n","        matrix [x, 0] = seq1[x-1]\n","    for y in range(1, size_y):\n","        matrix [0, y] = seq2[y-1]\n","        \n","    #print(matrix)\n","    \n","    return matrix[size_x - 1, size_y - 1]"]},{"cell_type":"markdown","metadata":{"id":"ysUc_UcZgw6Z"},"source":["In order for this to work, we must also load a list of countries for reference using the country_list module."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"a3rVLxzcgw6Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664477330046,"user_tz":240,"elapsed":100,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"c737b26e-f698-44a9-82d3-7b75b6d94ba2"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Afghanistan', 'Åland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua & Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia', 'Bosnia & Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'British Virgin Islands', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Caribbean Netherlands', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo - Brazzaville', 'Congo - Kinshasa', 'Cook Islands', 'Costa Rica', 'Côte d’Ivoire', 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czechia', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Eswatini', 'Ethiopia', 'Falkland Islands', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard & McDonald Islands', 'Honduras', 'Hong Kong SAR China', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao SAR China', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar (Burma)', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'North Korea', 'North Macedonia', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territories', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn Islands', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Réunion', 'Romania', 'Russia', 'Rwanda', 'Samoa', 'San Marino', 'São Tomé & Príncipe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia & South Sandwich Islands', 'South Korea', 'South Sudan', 'Spain', 'Sri Lanka', 'St. Barthélemy', 'St. Helena', 'St. Kitts & Nevis', 'St. Lucia', 'St. Martin', 'St. Pierre & Miquelon', 'St. Vincent & Grenadines', 'Sudan', 'Suriname', 'Svalbard & Jan Mayen', 'Sweden', 'Switzerland', 'Syria', 'Taiwan', 'Tajikistan', 'Tanzania', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad & Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks & Caicos Islands', 'Tuvalu', 'U.S. Outlying Islands', 'U.S. Virgin Islands', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Vatican City', 'Venezuela', 'Vietnam', 'Wallis & Futuna', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe']\n"]}],"source":["countries = list(dict(countries_for_language('en')).values())\n","print(countries)"]},{"cell_type":"markdown","metadata":{"id":"pxWBFdY8gw6a"},"source":["Looking at the country names, it is evident that some of them may be difficult to spell correctly. Therefore, one way we can use the Levenshtein Distance is to find the correct spelling of a country name by finding the one that is closest our misspelling. See below for an example."]},{"cell_type":"code","execution_count":38,"metadata":{"id":"eum3s_j8gw6a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664480390069,"user_tz":240,"elapsed":218,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"32772233-6b07-4e8c-d434-d05fd593ddfc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Aruba', 3), ('Austria', 3), ('Cuba', 3)]"]},"metadata":{},"execution_count":38}],"source":["# function to calculatedisrance\n","def find_closest_country(country_mispelling):\n","    distances = {}\n","    for country in countries:\n","        distance = levenshtein(country_mispelling, country)\n","        distances[country] = distance\n","    return sorted(distances.items(), key=lambda c:c[1])[0:3] # return the three closest countries\n","\n","find_closest_country(\"Aust\")"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"BDWvgAKXgw6b","executionInfo":{"status":"ok","timestamp":1664477330143,"user_tz":240,"elapsed":10,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["### YOUR CODE: try this with other common country mispellings\n","\n","### for inspiration: https://www.funtrivia.com/playquiz/quiz32051824b17d8.html"]},{"cell_type":"markdown","metadata":{"id":"kkrG4Ffzgw6b"},"source":["In addition to helping identify the correct spelling of a word, Levenshtein Distance can help us with auto-completion tasks. See below for an example."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"twl45RkIgw6b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664477330143,"user_tz":240,"elapsed":9,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"0b587966-664c-406f-d7b8-d20c275c4535"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Sri Lanka', 3), ('Syria', 3), ('Aruba', 4)]"]},"metadata":{},"execution_count":20}],"source":["find_closest_country(\"Sri La\")"]},{"cell_type":"markdown","metadata":{"id":"OeUwd135gw6b"},"source":["### 1.2 BLAST: Basic Local Alignment Search Tool\n","\n","We can think of BLAST as a heuristic model for performing local alignment tasks that operates by searching for high scoring segment pairs (HSPs). As discussed in lecture, BLAST views sequences as sequences of short words (k-tuple). Therefore, in the case of good alignment, we should see close matches between many of these sub-sequences, or HSPs. To showcase BLAST, we will utilize an online version provided through NCBI."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"bkEyVtqrgw6c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664477330143,"user_tz":240,"elapsed":7,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"5b726c7c-9e38-407c-a9f9-d6ed67c73d6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Help on function qblast in module Bio.Blast.NCBIWWW:\n","\n","qblast(program, database, sequence, url_base='https://blast.ncbi.nlm.nih.gov/Blast.cgi', auto_format=None, composition_based_statistics=None, db_genetic_code=None, endpoints=None, entrez_query='(none)', expect=10.0, filter=None, gapcosts=None, genetic_code=None, hitlist_size=50, i_thresh=None, layout=None, lcase_mask=None, matrix_name=None, nucl_penalty=None, nucl_reward=None, other_advanced=None, perc_ident=None, phi_pattern=None, query_file=None, query_believe_defline=None, query_from=None, query_to=None, searchsp_eff=None, service=None, threshold=None, ungapped_alignment=None, word_size=None, short_query=None, alignments=500, alignment_view=None, descriptions=500, entrez_links_new_window=None, expect_low=None, expect_high=None, format_entrez_query=None, format_object=None, format_type='XML', ncbi_gi=None, results_file=None, show_overview=None, megablast=None, template_type=None, template_length=None)\n","    BLAST search using NCBI's QBLAST server or a cloud service provider.\n","    \n","    Supports all parameters of the old qblast API for Put and Get.\n","    \n","    Please note that NCBI uses the new Common URL API for BLAST searches\n","    on the internet (http://ncbi.github.io/blast-cloud/dev/api.html). Thus,\n","    some of the parameters used by this function are not (or are no longer)\n","    officially supported by NCBI. Although they are still functioning, this\n","    may change in the future.\n","    \n","    The Common URL API (http://ncbi.github.io/blast-cloud/dev/api.html) allows\n","    doing BLAST searches on cloud servers. To use this feature, please set\n","    ``url_base='http://host.my.cloud.service.provider.com/cgi-bin/blast.cgi'``\n","    and ``format_object='Alignment'``. For more details, please see\n","    https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE_TYPE=BlastDocs&DOC_TYPE=CloudBlast\n","    \n","    Some useful parameters:\n","    \n","     - program        blastn, blastp, blastx, tblastn, or tblastx (lower case)\n","     - database       Which database to search against (e.g. \"nr\").\n","     - sequence       The sequence to search.\n","     - ncbi_gi        TRUE/FALSE whether to give 'gi' identifier.\n","     - descriptions   Number of descriptions to show.  Def 500.\n","     - alignments     Number of alignments to show.  Def 500.\n","     - expect         An expect value cutoff.  Def 10.0.\n","     - matrix_name    Specify an alt. matrix (PAM30, PAM70, BLOSUM80, BLOSUM45).\n","     - filter         \"none\" turns off filtering.  Default no filtering\n","     - format_type    \"HTML\", \"Text\", \"ASN.1\", or \"XML\".  Def. \"XML\".\n","     - entrez_query   Entrez query to limit Blast search\n","     - hitlist_size   Number of hits to return. Default 50\n","     - megablast      TRUE/FALSE whether to use MEga BLAST algorithm (blastn only)\n","     - short_query    TRUE/FALSE whether to adjust the search parameters for a\n","                      short query sequence. Note that this will override\n","                      manually set parameters like word size and e value. Turns\n","                      off when sequence length is > 30 residues. Default: None.\n","     - service        plain, psi, phi, rpsblast, megablast (lower case)\n","    \n","    This function does no checking of the validity of the parameters\n","    and passes the values to the server as is.  More help is available at:\n","    https://ncbi.github.io/blast-cloud/dev/api.html\n","\n"]}],"source":["# sources: \n","# https://www.tutorialspoint.com/biopython/biopython_overview_of_blast.htm\n","# https://blast.ncbi.nlm.nih.gov/Blast.cgi\n","\n","help(NCBIWWW.qblast) # help documentation"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"-XCBGdICgw6c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664477392290,"user_tz":240,"elapsed":62150,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"494d1fc5-2297-4288-d2ac-e293e5d77e51"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_io.StringIO at 0x7fd8f5a79d70>"]},"metadata":{},"execution_count":22}],"source":["# example running qblast\n","\n","# WARNING: CAN TAKE ~5-10 minutes to run depending on load\n","\n","# example of poliovirus\n","# blastn = nucleotide search program\n","# nt = nucleotide database \n","# sequence = GI number of the sequence (poliovirus in this case) = 12408699 \n","\n","result_handle = NCBIWWW.qblast(\"blastn\", \"nt\", sequence = 12408699) \n","result_handle "]},{"cell_type":"code","execution_count":23,"metadata":{"id":"MKYS1MHDgw6c","executionInfo":{"status":"ok","timestamp":1664477392291,"user_tz":240,"elapsed":33,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# write the results to a file\n","\n","with open('polio_results.xml', 'w') as results_file: \n","    results = result_handle.read() \n","    results_file.write(results)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"XIiM9Wlugw6c","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664477392291,"user_tz":240,"elapsed":32,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"dddcf030-5f16-47e0-dbb5-d230aa0bbcf4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Match: gi|61252|emb|V01149.1| Human poliovirus 1 Mahoney, complete genome \n","\n","Match: gi|1012282727|gb|KU866422.1| Human poliovirus 1 strain Mahoney_CDC, complete genome \n","\n","Match: gi|27085396|gb|AY184219.1| Human poliovirus 1 strain Sabin 1, complete genome \n","\n","Match: gi|283137719|gb|GQ984141.1| Human poliovirus 1 strain Sabin 1 isolate S302, complete genome \n","\n","Match: gi|61236|emb|V01148.1| Genome of human poliovirus type 1 (Mahoney strain). (One of two versions.) \n","\n","Match: gi|61257|emb|V01150.1| Human poliovirus strain Sabin 1 complete genome, strain Sabin 1 \n","\n","Match: gi|987390362|gb|KT353719.1| Human poliovirus 1 strain 1-B2, complete genome \n","\n","Match: gi|18644085|emb|AJ430385.1| Human poliovirus 1 genomic RNA for polyprotein gene, strain Cox \n","\n","Match: gi|193245070|gb|EU794954.1| Human poliovirus 1 isolate A49 polyprotein gene, complete cds \n","\n","Match: gi|643433715|gb|KJ170502.1| Human poliovirus 1 strain NIE0918388, complete genome >gi|643433717|gb|KJ170503.1| Human poliovirus 1 strain NIE0918389, complete genome \n","\n","Match: gi|643433711|gb|KJ170500.1| Human poliovirus 1 strain NIE0918315, complete genome \n","\n","Match: gi|643433629|gb|KJ170459.1| Human poliovirus 1 strain NIE1118378, complete genome >gi|643433631|gb|KJ170460.1| Human poliovirus 1 strain NIE1118331, complete genome >gi|643433633|gb|KJ170461.1| Human poliovirus 1 strain NIE1118336, complete genome >gi|643433635|gb|KJ170462.1| Human poliovirus 1 strain NIE1118340, complete genome >gi|643433637|gb|KJ170463.1| Human poliovirus 1 strain NIE1218342, complete genome >gi|643433639|gb|KJ170464.1| Human poliovirus 1 strain NIE1218345, complete genome >gi|643433641|gb|KJ170465.1| Human poliovirus 1 strain NIE1218347, complete genome >gi|643433643|gb|KJ170466.1| Human poliovirus 1 strain NIE1218350, complete genome >gi|643433645|gb|KJ170467.1| Human poliovirus 1 strain NIE0918311, complete genome >gi|643433647|gb|KJ170468.1| Human poliovirus 1 strain NIE1118338, complete genome >gi|643433649|gb|KJ170469.1| Human poliovirus 1 strain NIE1218343, complete genome >gi|643433651|gb|KJ170470.1| Human poliovirus 1 strain NIE1018396, complete genome >gi|643433653|gb|KJ170471.1| Human poliovirus 1 strain NIE1118334, complete genome >gi|643433655|gb|KJ170472.1| Human poliovirus 1 strain NIE1118332, complete genome >gi|643433657|gb|KJ170473.1| Human poliovirus 1 strain NIE1118337, complete genome >gi|643433659|gb|KJ170474.1| Human poliovirus 1 strain NIE1118339, complete genome \n","\n","Match: gi|193245072|gb|EU794955.1| Human poliovirus 1 isolate A63 polyprotein gene, complete cds \n","\n","Match: gi|1402399317|gb|MG571844.1| MAG: Human poliovirus 1 clone V8A1 polyprotein gene, complete cds \n","\n","Match: gi|643433721|gb|KJ170505.1| Human poliovirus 1 strain NIE1118317, complete genome \n","\n","Match: gi|643433707|gb|KJ170498.1| Human poliovirus 1 strain NIE1218353, complete genome \n","\n","Match: gi|643433705|gb|KJ170497.1| Human poliovirus 1 strain NIE1218344, complete genome \n","\n","Match: gi|643433703|gb|KJ170496.1| Human poliovirus 1 strain NIE1218346, complete genome \n","\n","Match: gi|643433695|gb|KJ170492.1| Human poliovirus 1 strain NIE1118400, complete genome \n","\n","Match: gi|643433691|gb|KJ170490.1| Human poliovirus 1 strain NIE1118333, complete genome \n","\n","Match: gi|643433685|gb|KJ170487.1| Human poliovirus 1 strain NIE1118318, complete genome \n","\n","Match: gi|643433679|gb|KJ170484.1| Human poliovirus 1 strain NIE0918312, complete genome \n","\n","Match: gi|643433675|gb|KJ170482.1| Human poliovirus 1 strain NIE1218341, complete genome \n","\n","Match: gi|643433673|gb|KJ170481.1| Human poliovirus 1 strain NIE1118330, complete genome \n","\n","Match: gi|643433671|gb|KJ170480.1| Human poliovirus 1 strain NIE1118328, complete genome \n","\n","Match: gi|643433665|gb|KJ170477.1| Human poliovirus 1 strain NIE0918314, complete genome \n","\n","Match: gi|643433661|gb|KJ170475.1| Human poliovirus 1 strain NIE1018320, complete genome \n","\n","Match: gi|643433619|gb|KJ170454.1| Human poliovirus 1 strain NIE1118401, complete genome \n","\n","Match: gi|643433613|gb|KJ170451.1| Human poliovirus 1 strain NIE1018321, complete genome \n","\n","Match: gi|643433607|gb|KJ170448.1| Human poliovirus 1 strain NIE1118376, complete genome >gi|643433609|gb|KJ170449.1| Human poliovirus 1 strain NIE1118368, complete genome \n","\n","Match: gi|643433727|gb|KJ170508.1| Human poliovirus 1 strain NIE1018382, complete genome \n","\n","Match: gi|643433713|gb|KJ170501.1| Human poliovirus 1 strain NIE1018393, complete genome \n","\n","Match: gi|643433687|gb|KJ170488.1| Human poliovirus 1 strain NIE1018325, complete genome \n","\n","Match: gi|643433683|gb|KJ170486.1| Human poliovirus 1 strain NIE0918380, complete genome \n","\n","Match: gi|643433681|gb|KJ170485.1| Human poliovirus 1 strain NIE1218348, complete genome \n","\n","Match: gi|643433677|gb|KJ170483.1| Human poliovirus 1 strain NIE1218349, complete genome \n","\n","Match: gi|643433669|gb|KJ170479.1| Human poliovirus 1 strain NIE1118386, complete genome \n","\n","Match: gi|643433667|gb|KJ170478.1| Human poliovirus 1 strain NIE0918391, complete genome \n","\n","Match: gi|643433663|gb|KJ170476.1| Human poliovirus 1 strain NIE1118404, complete genome \n","\n","Match: gi|643433627|gb|KJ170458.1| Human poliovirus 1 strain NIE1018322, complete genome \n","\n","Match: gi|643433623|gb|KJ170456.1| Human poliovirus 1 strain NIE1118403, complete genome \n","\n","Match: gi|643433617|gb|KJ170453.1| Human poliovirus 1 strain NIE1118398, complete genome \n","\n","Match: gi|643433615|gb|KJ170452.1| Human poliovirus 1 strain NIE1118397, complete genome \n","\n","Match: gi|643433605|gb|KJ170447.1| Human poliovirus 1 strain NIE1118369, complete genome \n","\n","Match: gi|643433603|gb|KJ170446.1| Human poliovirus 1 strain NIE1018366, complete genome \n","\n","Match: gi|643433593|gb|KJ170441.1| Human poliovirus 1 strain NIE1018362, complete genome >gi|643433595|gb|KJ170442.1| Human poliovirus 1 strain NIE1018364, complete genome >gi|643433597|gb|KJ170443.1| Human poliovirus 1 strain NIE1118406, complete genome >gi|643433599|gb|KJ170444.1| Human poliovirus 1 strain NIE1018361, complete genome \n","\n","Match: gi|643433589|gb|KJ170439.1| Human poliovirus 1 strain NIE1018354, complete genome \n","\n","Match: gi|643433585|gb|KJ170437.1| Human poliovirus 1 strain NIE1218351, complete genome \n","\n","Match: gi|193245068|gb|EU794953.1| Human poliovirus 1 isolate A21 polyprotein gene, complete cds \n","\n","Match: gi|643433759|gb|KJ170524.1| Human poliovirus 1 strain NIE0918390, complete genome \n","\n"]}],"source":["# look through the results\n","\n","threshold = 1e-20 # think of this like a p-value\n","for record in NCBIXML.parse(open(\"polio_results.xml\")): \n","     if record.alignments: \n","        for align in record.alignments: \n","            for hsp in align.hsps: \n","                if hsp.expect < threshold: \n","                    print(\"Match:\", align.title, \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"iHfYKI33gw6d"},"source":["From running this query, we see that we get back a bunch of different samples of poliovirus variations."]},{"cell_type":"code","execution_count":25,"metadata":{"id":"1OorzuUGgw6d","executionInfo":{"status":"ok","timestamp":1664477392297,"user_tz":240,"elapsed":35,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["### YOUR CODE: try this out with another nucleotide sequence\n","\n","\n","### inspiration: find the GI number of a sequence using https://www.ncbi.nlm.nih.gov/nuccore"]},{"cell_type":"markdown","metadata":{"id":"aW96bGC_gw6d"},"source":["## 2. Sequence Prediction/Labeling\n","\n","For this part of the lab, we will be experimenting with different models to perform Named Entity Recognition (NER). The goal of NER is to label items into a set of predefined states/entities, such as an organizations, dates/times, geographical locations, etc. To do so, we will use a dummy majority classifier as our baseline and then will experiment with two sequence models introduced in lecture: Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs)."]},{"cell_type":"markdown","metadata":{"id":"pSDVmohigw6d"},"source":["### 2.1 Data Exploration, Data Preparation & Train-Test Splitting"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"nrlfqNFrgw6d","colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"status":"error","timestamp":1664477392306,"user_tz":240,"elapsed":43,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}},"outputId":"29b4a589-8b01-42b3-81d1-78845aa12e8d"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-2ee190b501db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# data source: https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mner_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ner_dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mner_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ner_dataset.csv'"]}],"source":["# load the data\n","\n","# data source: https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\n","ner_dataset = pd.read_csv('ner_dataset.csv', encoding = \"ISO-8859-1\")\n","print(ner_dataset.shape)\n","ner_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6LbEOWEgw6e","executionInfo":{"status":"aborted","timestamp":1664477392307,"user_tz":240,"elapsed":39,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# how many sentences?\n","print(\"Number of sentences:\", ner_dataset['Sentence #'].nunique())\n","\n","# distribution of tags?\n","ner_dataset['Tag'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"KQiGFcppgw6e"},"source":["Some notes about the data:\n","\n","We can think of 'O' as background or not a named entity. We see that 'O' is by far the most common state indicating that most text is not a named entity in this case.\n","\n","'B-' indicates the beginning of the named entity and 'I' indicates that it is a continued part of the named entity.\n","\n","Overall, we see that the distribution of the named entities is heavily skewed with some entities appearing much more than others in the data. This is important to keep in mind when fitting models and assessing performance.\n","\n","\n","Next, we will want to fill in the NaN values in the 'Sentence' column with the previous value so that we can attribute each word/tag to the correct sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8e9MhVdgw6e","executionInfo":{"status":"aborted","timestamp":1664477392307,"user_tz":240,"elapsed":39,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# fill in NaN sentences\n","ner_dataset.fillna(method='ffill', inplace=True)\n","ner_dataset.head()"]},{"cell_type":"markdown","metadata":{"id":"ZfRqzGeigw6e"},"source":["As this dataset is very large, we will only work with 2,000 sentences in our training set and 1,000 sentences in our testing set to reduce computational costs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMve3NJCgw6e","executionInfo":{"status":"aborted","timestamp":1664477392308,"user_tz":240,"elapsed":40,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["ner_train = ner_dataset[0:44396] # first 2000 sentences\n","ner_test = ner_dataset[44396:66174] # next 1000 sentences\n","\n","ner_X_train = ner_train.drop(columns=\"Tag\") # training features\n","ner_y_train = ner_train.Tag # training outcome variable\n","ner_X_test = ner_test.drop(columns=\"Tag\") # testing features\n","ner_y_test = ner_test.Tag # testing outcome variable\n","\n","# checks\n","print(ner_train.shape)\n","print(ner_X_train.shape)\n","print(ner_y_train.shape)\n","print(ner_test.shape)\n","print(ner_X_test.shape)\n","print(ner_y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"4cXF8M8Rgw6f"},"source":["### 2.2 Model Fitting"]},{"cell_type":"markdown","metadata":{"id":"jrSB9JiQgw6f"},"source":["#### 2.2.1 Dummy Majority Classifier\n","\n","The first model we will implement is a dummy classifier that will label each word with the most frequent entity, which is 'O' in this case. This will serve as our baseline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8AaMSOiogw6f","executionInfo":{"status":"aborted","timestamp":1664477392308,"user_tz":240,"elapsed":40,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# fit the model\n","\n","dc = DummyClassifier(strategy=\"most_frequent\") # initalize the model\n","dc.fit(ner_X_train, ner_y_train) # fit the model to the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4n_n_X_Cgw6f","executionInfo":{"status":"aborted","timestamp":1664477392308,"user_tz":240,"elapsed":40,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# assess performance using classification_report\n","\n","states = list(ner_y_train.unique()) # list of unique states in our training dataset\n","\n","dc_preds = dc.predict(ner_X_test) # make predictions for test data\n","print(classification_report(y_true = ner_y_test, y_pred = dc_preds, labels = states)) # assess performance by hidden state"]},{"cell_type":"markdown","metadata":{"id":"FMPhLZgMgw6f"},"source":["Overall, we see that this baseline model only performs well for 'O' since it is predicting 'O' for every observation. This shows that we must be extra careful when interpreting the results since the large representation of 'O' in the data will bias the overall results (i.e., the micro/macro/weighted avgs). This is is why we cannot just look at the overall results and must assess performance by hidden state as well."]},{"cell_type":"markdown","metadata":{"id":"FO1cJzUJgw6g"},"source":["#### 2.2.2 HMMs\n","Next we will implement a Hidden Markov Model, or HMM. As review, for HMMs, we have (1) a set of observed states, (2) a set of hidden states, (3) initial state probabilities, (4) state transition probabilities, and (5) observation state probabilities. HMMs are a generative model meaning that we can think of the the hidden states as \"generating\" the observations. This can help explain why a particular sequence was observed by finding the most likely path that generated the observations. However, it is important to remember that with HMMs, the hidden states are only dependent on the previous state and the observed states are only dependent on its hidden state. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bXs7qSh3gw6g","executionInfo":{"status":"aborted","timestamp":1664477392309,"user_tz":240,"elapsed":40,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# source: https://towardsdatascience.com/part-of-speech-tagging-with-hidden-markov-chain-models-e9fccc835c0e\n","from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbRIfyDUgw6g","executionInfo":{"status":"aborted","timestamp":1664477392309,"user_tz":240,"elapsed":40,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\") # intialize our model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5XWem8olgw6g","executionInfo":{"status":"aborted","timestamp":1664477392309,"user_tz":240,"elapsed":40,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["tags = list(ner_train.Tag) # list of tags\n","words = list(ner_train.Word) # list of words\n","words = [word.lower() for word in words] # lowercase all words\n","\n","tags_count = Counter(tags) # frequency of each tag\n","\n","tag_combos = [(tags[i],tags[i+1]) for i in range(0,len(tags)-2,2)] # look at each pair of tags\n","tag_bigrams = Counter(tag_combos) # count the frequency of each pair of tags"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8Qts19ygw6g","executionInfo":{"status":"aborted","timestamp":1664477392310,"user_tz":240,"elapsed":40,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["starts_tag = ner_train.drop_duplicates(subset = 'Sentence #', keep=\"first\").Tag # starting tags\n","starting_tag_count = Counter(starts_tag) # frequency of each starting tag\n","\n","end_tag = ner_train.drop_duplicates(subset = 'Sentence #', keep=\"last\").Tag # ending tags\n","ending_tag_count = Counter(end_tag) # frequency of each ending tag"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ucJG_Ahzgw6g","executionInfo":{"status":"aborted","timestamp":1664477392310,"user_tz":240,"elapsed":40,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# function to calculate the frequency of each tag/word combo\n","\n","def pair_counts(tags, words):\n","    counts = defaultdict(lambda: defaultdict(int))\n","    for tag, word in zip(tags, words):\n","        counts[tag][word] += 1\n","    return counts\n","\n","tag_words_count = pair_counts(tags, words) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lW-FlLEGgw6h","executionInfo":{"status":"aborted","timestamp":1664477392310,"user_tz":240,"elapsed":40,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# probability distribution of words at each state\n","\n","test_words = list(ner_test.Word) # list of test words\n","test_words = [word.lower() for word in test_words] # lowercase all words\n","all_words = set(words + test_words) # unique set of all words in train & test combined\n","\n","to_pass_states = []\n","for tag, words_dict in tag_words_count.items():\n","    total = float(sum(words_dict.values()))\n","    distribution = {word: count/total for word, count in words_dict.items()}\n","    \n","    # add default probability for all words (in train and/or test) to prevent errors during prediction\n","    for word in all_words:\n","        if word not in list(distribution.keys()):\n","                distribution[word] = 1/1000\n","    \n","    tag_emissions = DiscreteDistribution(distribution)\n","    tag_state = State(tag_emissions, name=tag)\n","    to_pass_states.append(tag_state)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmrB9i6Tgw6h","executionInfo":{"status":"aborted","timestamp":1664477392311,"user_tz":240,"elapsed":41,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["basic_model.add_states()\n","\n","# start probability for each state\n","start_prob={}\n","for tag in tags:\n","    start_prob[tag] = starting_tag_count[tag]/tags_count[tag]\n","    \n","for tag_state in to_pass_states:\n","    basic_model.add_transition(basic_model.start,tag_state,start_prob[tag_state.name])\n","\n","# end probability for each state\n","end_prob={}\n","for tag in tags:\n","    end_prob[tag]=ending_tag_count[tag]/tags_count[tag]\n","    \n","for tag_state in to_pass_states:\n","    basic_model.add_transition(tag_state,basic_model.end,end_prob[tag_state.name])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_o2UQlCgw6h","executionInfo":{"status":"aborted","timestamp":1664477392311,"user_tz":240,"elapsed":41,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# transition probabilities between states\n","\n","transition_prob_pair = {}\n","for key in tag_bigrams.keys():\n","    transition_prob_pair[key] = tag_bigrams.get(key)/tags_count[key[0]]\n","    \n","for tag_state in to_pass_states :\n","    for next_tag_state in to_pass_states:\n","        try:\n","            transition = transition_prob_pair[(tag_state.name,next_tag_state.name)]\n","        except:\n","            transition = 0 # if transition is not found in data\n","        basic_model.add_transition(tag_state,next_tag_state,transition)\n","        \n","basic_model.bake() # finalize model"]},{"cell_type":"markdown","metadata":{"id":"sGPlUOyGgw6h"},"source":["Now that we have created the model, we can use it to make predictions on the test data. To do so, we will create a nested list of our test sentences where each sentence is its own list. Then, we will make our predictions on each sentence individually."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iCkfUbUGgw6h","executionInfo":{"status":"aborted","timestamp":1664477392311,"user_tz":240,"elapsed":40,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# get list of test sentences\n","\n","sentences_dict_test = {}\n","\n","for index, row in ner_test.iterrows():\n","    if row['Sentence #'] not in sentences_dict_test:\n","        sentences_dict_test[row['Sentence #']] = []\n","    sentences_dict_test[row['Sentence #']].append((row['Word'].lower()))\n","    \n","sentences_test = list(sentences_dict_test.values())\n","print(sentences_test[1]) # check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7e4HEAe_gw6h","executionInfo":{"status":"aborted","timestamp":1664477392322,"user_tz":240,"elapsed":51,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# make predictions by the sentence\n","\n","hmm_preds = []\n","for sentence in sentences_test:\n","    _, state_path = basic_model.viterbi(sentence)\n","    tags = [state[1].name for state in state_path[1:-1]]\n","    hmm_preds.append(tags)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"689JMDO7gw6i","executionInfo":{"status":"aborted","timestamp":1664477392323,"user_tz":240,"elapsed":52,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# assess performance\n","\n","states = list(ner_y_train.unique()) # list of unique states in our dataset\n","\n","print(classification_report(y_true = ner_y_test, y_pred = hmm_preds, labels = states))"]},{"cell_type":"markdown","metadata":{"id":"3pRO6qIAgw6i"},"source":["We see that the results are better than our baseline model but only slightly. This can be at least partially attributed to the fact that we had to add default probabilities (1/1000 in this case) for a large fraction of the word/tag combinations since they did not appear in our training data. Therefore, in order to improve performance in the future, we would want to train our model on substantially more data so that the training vocabulary is more expansive and representative."]},{"cell_type":"markdown","metadata":{"id":"l_rKtW4Dgw6i"},"source":["#### 2.2.3 CRFs\n","Lastly, we will implement a Conditional Random Field, or CRF for short. Contrary to HMMs, CRFs are a discriminative model with more relaxed assumptions of the features since we do not need to model the joint probability, P(X,Y) where X=states and y=labels. This means we can utilize more features in our models including past and future observations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hrur_4qJgw6i","executionInfo":{"status":"aborted","timestamp":1664477392323,"user_tz":240,"elapsed":52,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# transform training data into list of sentences with each row as a tuple of the word, pos and tag\n","\n","sentences_dict_train = {}\n","\n","for index, row in ner_train.iterrows():\n","    if row['Sentence #'] not in sentences_dict_train:\n","        sentences_dict_train[row['Sentence #']] = []\n","    sentences_dict_train[row['Sentence #']].append((row['Word'], row['POS'], row['Tag']))\n","    \n","sentences_train = sentences_dict_train.values()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uG2FFIzRgw6i","executionInfo":{"status":"aborted","timestamp":1664477392324,"user_tz":240,"elapsed":52,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# feature extraction\n","\n","# source: https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html\n","\n","def word2features(sent, i):\n","    word = sent[i][0]\n","    postag = sent[i][1]\n","    \n","    features = {\n","        'bias': 1.0, \n","        'word.lower()': word.lower(), \n","        'word[-3:]': word[-3:],\n","        'word[-2:]': word[-2:],\n","        'word.isupper()': word.isupper(),\n","        'word.istitle()': word.istitle(),\n","        'word.isdigit()': word.isdigit(),\n","        'postag': postag,\n","        'postag[:2]': postag[:2],\n","    }\n","    if i > 0:\n","        word1 = sent[i-1][0]\n","        postag1 = sent[i-1][1]\n","        features.update({\n","            '-1:word.lower()': word1.lower(),\n","            '-1:word.istitle()': word1.istitle(),\n","            '-1:word.isupper()': word1.isupper(),\n","            '-1:postag': postag1,\n","            '-1:postag[:2]': postag1[:2],\n","        })\n","    else:\n","        features['BOS'] = True\n","    if i < len(sent)-1:\n","        word1 = sent[i+1][0]\n","        postag1 = sent[i+1][1]\n","        features.update({\n","            '+1:word.lower()': word1.lower(),\n","            '+1:word.istitle()': word1.istitle(),\n","            '+1:word.isupper()': word1.isupper(),\n","            '+1:postag': postag1,\n","            '+1:postag[:2]': postag1[:2],\n","        })\n","    else:\n","        features['EOS'] = True\n","    return features\n","\n","def sent2features(sent):\n","    return [word2features(sent, i) for i in range(len(sent))]\n","\n","def sent2labels(sent):\n","    return [label for token, postag, label in sent]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SkdNn8Lgw6i","executionInfo":{"status":"aborted","timestamp":1664477392324,"user_tz":240,"elapsed":52,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["X_train = [sent2features(s) for s in sentences_train] # converting training features\n","y_train = [sent2labels(s) for s in sentences_train] # converting tags\n","\n","print(len(X_train)) # check\n","print(len(y_train)) # check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CLby1BA1gw6j","executionInfo":{"status":"aborted","timestamp":1664477392325,"user_tz":240,"elapsed":53,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# fit the model\n","crf = sklearn_crfsuite.CRF() # initalize the model\n","crf.fit(X_train, y_train) # fit the model to the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgUQlYzBgw6j","executionInfo":{"status":"aborted","timestamp":1664477392325,"user_tz":240,"elapsed":53,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# transform testing data into list of sentences with each row as a tuple of the word, pos and tag\n","\n","sentences_dict_test = {}\n","\n","for index, row in ner_test.iterrows():\n","    if row['Sentence #'] not in sentences_dict_test:\n","        sentences_dict_test[row['Sentence #']] = []\n","    sentences_dict_test[row['Sentence #']].append((row['Word'], row['POS'], row['Tag']))\n","    \n","sentences_test = sentences_dict_test.values()\n","\n","X_test = [sent2features(s) for s in sentences_test]\n","y_test = [sent2labels(s) for s in sentences_test]\n","\n","print(len(X_test)) # check\n","print(len(y_test)) # check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nV58_2zagw6j","scrolled":true,"executionInfo":{"status":"aborted","timestamp":1664477392325,"user_tz":240,"elapsed":53,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["# make predictions & assess performance\n","\n","crf_preds = crf.predict(X_test) \n","crf_preds = [item for sublist in crf_preds for item in sublist] # flatten out predictions\n","\n","print(classification_report(y_true = ner_y_test, y_pred = crf_preds, labels = states))"]},{"cell_type":"markdown","metadata":{"id":"DJBJDe-Rgw6j"},"source":["We see that the CRF model performs the best! Let's see if we can improve performance even more."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUP9KE93gw6j","executionInfo":{"status":"aborted","timestamp":1664477392326,"user_tz":240,"elapsed":53,"user":{"displayName":"Jiawen Zhang (Robin)","userId":"03456407888046212080"}}},"outputs":[],"source":["### YOUR CODE: play around with the CRF features/parameters - can you improve performance even more?\n","\n","### For inspiration, check out https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html#let-s-use-conll-2002-data-to-build-a-ner-system"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"video-info":{"admin":["localhost:8888"],"hub-user":"localhost:8888","path":"Labs>>Lab 5 - Sequence Mining/Lab4_Sequences-StarterCode.ipynb","tpc":"localhost:8888@Labs>>Lab 5 - Sequence Mining/Lab4_Sequences-StarterCode.ipynb"}},"nbformat":4,"nbformat_minor":0}